from typing import List

from mflux.models.common.weights.mapping.weight_mapping import WeightMapping, WeightTarget
from mflux.models.qwen.weights.qwen_weight_mapping import (
    reshape_gamma_to_1d,
    transpose_conv2d_weight,
    transpose_conv3d_weight,
)


class FIBOWeightMapping(WeightMapping):
    @staticmethod
    def get_transformer_mapping() -> List[WeightTarget]:
        return [
            # ========== Global projections ==========
            WeightTarget(
                mlx_path="time_embed.timestep_embedder.linear_1.weight",
                hf_patterns=["time_embed.timestep_embedder.linear_1.weight"],
            ),
            WeightTarget(
                mlx_path="time_embed.timestep_embedder.linear_1.bias",
                hf_patterns=["time_embed.timestep_embedder.linear_1.bias"],
            ),
            WeightTarget(
                mlx_path="time_embed.timestep_embedder.linear_2.weight",
                hf_patterns=["time_embed.timestep_embedder.linear_2.weight"],
            ),
            WeightTarget(
                mlx_path="time_embed.timestep_embedder.linear_2.bias",
                hf_patterns=["time_embed.timestep_embedder.linear_2.bias"],
            ),
            WeightTarget(
                mlx_path="context_embedder.weight",
                hf_patterns=["context_embedder.weight"],
            ),
            WeightTarget(
                mlx_path="context_embedder.bias",
                hf_patterns=["context_embedder.bias"],
            ),
            WeightTarget(
                mlx_path="x_embedder.weight",
                hf_patterns=["x_embedder.weight"],
            ),
            WeightTarget(
                mlx_path="x_embedder.bias",
                hf_patterns=["x_embedder.bias"],
            ),
            # ========== Joint transformer blocks ==========
            # AdaLayerNormZero (image + context streams)
            WeightTarget(
                mlx_path="transformer_blocks.{block}.norm1.linear.weight",
                hf_patterns=["transformer_blocks.{block}.norm1.linear.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.norm1.linear.bias",
                hf_patterns=["transformer_blocks.{block}.norm1.linear.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.norm1_context.linear.weight",
                hf_patterns=["transformer_blocks.{block}.norm1_context.linear.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.norm1_context.linear.bias",
                hf_patterns=["transformer_blocks.{block}.norm1_context.linear.bias"],
            ),
            # Attention weights (BriaFiboAttention)
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.norm_q.weight",
                hf_patterns=["transformer_blocks.{block}.attn.norm_q.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.norm_k.weight",
                hf_patterns=["transformer_blocks.{block}.attn.norm_k.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.to_q.weight",
                hf_patterns=["transformer_blocks.{block}.attn.to_q.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.to_q.bias",
                hf_patterns=["transformer_blocks.{block}.attn.to_q.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.to_k.weight",
                hf_patterns=["transformer_blocks.{block}.attn.to_k.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.to_k.bias",
                hf_patterns=["transformer_blocks.{block}.attn.to_k.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.to_v.weight",
                hf_patterns=["transformer_blocks.{block}.attn.to_v.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.to_v.bias",
                hf_patterns=["transformer_blocks.{block}.attn.to_v.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.to_out.0.weight",
                hf_patterns=["transformer_blocks.{block}.attn.to_out.0.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.to_out.0.bias",
                hf_patterns=["transformer_blocks.{block}.attn.to_out.0.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.norm_added_q.weight",
                hf_patterns=["transformer_blocks.{block}.attn.norm_added_q.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.norm_added_k.weight",
                hf_patterns=["transformer_blocks.{block}.attn.norm_added_k.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.add_q_proj.weight",
                hf_patterns=["transformer_blocks.{block}.attn.add_q_proj.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.add_q_proj.bias",
                hf_patterns=["transformer_blocks.{block}.attn.add_q_proj.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.add_k_proj.weight",
                hf_patterns=["transformer_blocks.{block}.attn.add_k_proj.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.add_k_proj.bias",
                hf_patterns=["transformer_blocks.{block}.attn.add_k_proj.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.add_v_proj.weight",
                hf_patterns=["transformer_blocks.{block}.attn.add_v_proj.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.add_v_proj.bias",
                hf_patterns=["transformer_blocks.{block}.attn.add_v_proj.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.to_add_out.weight",
                hf_patterns=["transformer_blocks.{block}.attn.to_add_out.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.attn.to_add_out.bias",
                hf_patterns=["transformer_blocks.{block}.attn.to_add_out.bias"],
            ),
            # LayerNorm / FFN for image stream
            WeightTarget(
                mlx_path="transformer_blocks.{block}.norm2.weight",
                hf_patterns=["transformer_blocks.{block}.norm2.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.norm2.bias",
                hf_patterns=["transformer_blocks.{block}.norm2.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.ff.net.0.proj.weight",
                hf_patterns=["transformer_blocks.{block}.ff.net.0.proj.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.ff.net.0.proj.bias",
                hf_patterns=["transformer_blocks.{block}.ff.net.0.proj.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.ff.net.2.weight",
                hf_patterns=["transformer_blocks.{block}.ff.net.2.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.ff.net.2.bias",
                hf_patterns=["transformer_blocks.{block}.ff.net.2.bias"],
            ),
            # LayerNorm / FFN for context stream
            WeightTarget(
                mlx_path="transformer_blocks.{block}.norm2_context.weight",
                hf_patterns=["transformer_blocks.{block}.norm2_context.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.norm2_context.bias",
                hf_patterns=["transformer_blocks.{block}.norm2_context.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.ff_context.net.0.proj.weight",
                hf_patterns=["transformer_blocks.{block}.ff_context.net.0.proj.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.ff_context.net.0.proj.bias",
                hf_patterns=["transformer_blocks.{block}.ff_context.net.0.proj.bias"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.ff_context.net.2.weight",
                hf_patterns=["transformer_blocks.{block}.ff_context.net.2.weight"],
            ),
            WeightTarget(
                mlx_path="transformer_blocks.{block}.ff_context.net.2.bias",
                hf_patterns=["transformer_blocks.{block}.ff_context.net.2.bias"],
            ),
            # ========== Single transformer blocks ==========
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.norm.linear.weight",
                hf_patterns=["single_transformer_blocks.{block}.norm.linear.weight"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.norm.linear.bias",
                hf_patterns=["single_transformer_blocks.{block}.norm.linear.bias"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.attn.norm_q.weight",
                hf_patterns=["single_transformer_blocks.{block}.attn.norm_q.weight"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.attn.norm_k.weight",
                hf_patterns=["single_transformer_blocks.{block}.attn.norm_k.weight"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.attn.to_q.weight",
                hf_patterns=["single_transformer_blocks.{block}.attn.to_q.weight"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.attn.to_q.bias",
                hf_patterns=["single_transformer_blocks.{block}.attn.to_q.bias"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.attn.to_k.weight",
                hf_patterns=["single_transformer_blocks.{block}.attn.to_k.weight"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.attn.to_k.bias",
                hf_patterns=["single_transformer_blocks.{block}.attn.to_k.bias"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.attn.to_v.weight",
                hf_patterns=["single_transformer_blocks.{block}.attn.to_v.weight"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.attn.to_v.bias",
                hf_patterns=["single_transformer_blocks.{block}.attn.to_v.bias"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.proj_mlp.weight",
                hf_patterns=["single_transformer_blocks.{block}.proj_mlp.weight"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.proj_mlp.bias",
                hf_patterns=["single_transformer_blocks.{block}.proj_mlp.bias"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.proj_out.weight",
                hf_patterns=["single_transformer_blocks.{block}.proj_out.weight"],
            ),
            WeightTarget(
                mlx_path="single_transformer_blocks.{block}.proj_out.bias",
                hf_patterns=["single_transformer_blocks.{block}.proj_out.bias"],
            ),
            # ========== Caption projection & output head ==========
            WeightTarget(
                mlx_path="norm_out.linear.weight",
                hf_patterns=["norm_out.linear.weight"],
            ),
            WeightTarget(
                mlx_path="norm_out.linear.bias",
                hf_patterns=["norm_out.linear.bias"],
            ),
            WeightTarget(
                mlx_path="proj_out.weight",
                hf_patterns=["proj_out.weight"],
            ),
            WeightTarget(
                mlx_path="proj_out.bias",
                hf_patterns=["proj_out.bias"],
            ),
            # Caption_projection layers: we rely on explicit layer index to be
            # expanded via num_layers; we pass num_layers explicitly from the loader.
            WeightTarget(
                mlx_path="caption_projection.{layer}.linear.weight",
                hf_patterns=["caption_projection.{layer}.linear.weight"],
            ),
        ]

    @staticmethod
    def get_text_encoder_mapping() -> List[WeightTarget]:
        return [
            WeightTarget(
                mlx_path="embed_tokens.weight",
                hf_patterns=["model.embed_tokens.weight"],
            ),
            WeightTarget(
                mlx_path="layers.{block}.self_attn.q_proj.weight",
                hf_patterns=["model.layers.{block}.self_attn.q_proj.weight"],
            ),
            WeightTarget(
                mlx_path="layers.{block}.self_attn.k_proj.weight",
                hf_patterns=["model.layers.{block}.self_attn.k_proj.weight"],
            ),
            WeightTarget(
                mlx_path="layers.{block}.self_attn.v_proj.weight",
                hf_patterns=["model.layers.{block}.self_attn.v_proj.weight"],
            ),
            WeightTarget(
                mlx_path="layers.{block}.self_attn.o_proj.weight",
                hf_patterns=["model.layers.{block}.self_attn.o_proj.weight"],
            ),
            WeightTarget(
                mlx_path="layers.{block}.mlp.gate_proj.weight",
                hf_patterns=["model.layers.{block}.mlp.gate_proj.weight"],
            ),
            WeightTarget(
                mlx_path="layers.{block}.mlp.up_proj.weight",
                hf_patterns=["model.layers.{block}.mlp.up_proj.weight"],
            ),
            WeightTarget(
                mlx_path="layers.{block}.mlp.down_proj.weight",
                hf_patterns=["model.layers.{block}.mlp.down_proj.weight"],
            ),
            WeightTarget(
                mlx_path="layers.{block}.input_layernorm.weight",
                hf_patterns=["model.layers.{block}.input_layernorm.weight"],
            ),
            WeightTarget(
                mlx_path="layers.{block}.post_attention_layernorm.weight",
                hf_patterns=["model.layers.{block}.post_attention_layernorm.weight"],
            ),
            WeightTarget(
                mlx_path="norm.weight",
                hf_patterns=["model.norm.weight"],
            ),
        ]

    @staticmethod
    def get_vae_mapping() -> List[WeightTarget]:
        return [
            # ========== Encoder conv_in ==========
            WeightTarget(
                mlx_path="encoder.conv_in.conv3d.weight",
                hf_patterns=["encoder.conv_in.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="encoder.conv_in.conv3d.bias",
                hf_patterns=["encoder.conv_in.bias"],
            ),
            # ========== Encoder down_blocks ==========
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.resnets.{res}.norm1.weight",
                hf_patterns=["encoder.down_blocks.{block}.resnets.{res}.norm1.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.resnets.{res}.conv1.conv3d.weight",
                hf_patterns=["encoder.down_blocks.{block}.resnets.{res}.conv1.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.resnets.{res}.conv1.conv3d.bias",
                hf_patterns=["encoder.down_blocks.{block}.resnets.{res}.conv1.bias"],
            ),
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.resnets.{res}.norm2.weight",
                hf_patterns=["encoder.down_blocks.{block}.resnets.{res}.norm2.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.resnets.{res}.conv2.conv3d.weight",
                hf_patterns=["encoder.down_blocks.{block}.resnets.{res}.conv2.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.resnets.{res}.conv2.conv3d.bias",
                hf_patterns=["encoder.down_blocks.{block}.resnets.{res}.conv2.bias"],
            ),
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.resnets.{res}.conv_shortcut.conv3d.weight",
                hf_patterns=["encoder.down_blocks.{block}.resnets.{res}.conv_shortcut.weight"],
                transform=transpose_conv3d_weight,
                required=False,
            ),
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.resnets.{res}.conv_shortcut.conv3d.bias",
                hf_patterns=["encoder.down_blocks.{block}.resnets.{res}.conv_shortcut.bias"],
                required=False,
            ),
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.downsampler.resample_conv.weight",
                hf_patterns=["encoder.down_blocks.{block}.downsampler.resample.1.weight"],
                transform=transpose_conv2d_weight,
                required=False,
            ),
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.downsampler.resample_conv.bias",
                hf_patterns=["encoder.down_blocks.{block}.downsampler.resample.1.bias"],
                required=False,
            ),
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.downsampler.time_conv.conv3d.weight",
                hf_patterns=["encoder.down_blocks.{block}.downsampler.time_conv.weight"],
                transform=transpose_conv3d_weight,
                required=False,
            ),
            WeightTarget(
                mlx_path="encoder.down_blocks.{block}.downsampler.time_conv.conv3d.bias",
                hf_patterns=["encoder.down_blocks.{block}.downsampler.time_conv.bias"],
                required=False,
            ),
            # ========== Encoder mid_block ==========
            WeightTarget(
                mlx_path="encoder.mid_block.resnets.{i}.norm1.weight",
                hf_patterns=["encoder.mid_block.resnets.{i}.norm1.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="encoder.mid_block.resnets.{i}.conv1.conv3d.weight",
                hf_patterns=["encoder.mid_block.resnets.{i}.conv1.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="encoder.mid_block.resnets.{i}.conv1.conv3d.bias",
                hf_patterns=["encoder.mid_block.resnets.{i}.conv1.bias"],
            ),
            WeightTarget(
                mlx_path="encoder.mid_block.resnets.{i}.norm2.weight",
                hf_patterns=["encoder.mid_block.resnets.{i}.norm2.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="encoder.mid_block.resnets.{i}.conv2.conv3d.weight",
                hf_patterns=["encoder.mid_block.resnets.{i}.conv2.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="encoder.mid_block.resnets.{i}.conv2.conv3d.bias",
                hf_patterns=["encoder.mid_block.resnets.{i}.conv2.bias"],
            ),
            WeightTarget(
                mlx_path="encoder.mid_block.attentions.{i}.norm.weight",
                hf_patterns=["encoder.mid_block.attentions.{i}.norm.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="encoder.mid_block.attentions.{i}.to_qkv.weight",
                hf_patterns=["encoder.mid_block.attentions.{i}.to_qkv.weight"],
                transform=transpose_conv2d_weight,
            ),
            WeightTarget(
                mlx_path="encoder.mid_block.attentions.{i}.to_qkv.bias",
                hf_patterns=["encoder.mid_block.attentions.{i}.to_qkv.bias"],
            ),
            WeightTarget(
                mlx_path="encoder.mid_block.attentions.{i}.proj.weight",
                hf_patterns=["encoder.mid_block.attentions.{i}.proj.weight"],
                transform=transpose_conv2d_weight,
            ),
            WeightTarget(
                mlx_path="encoder.mid_block.attentions.{i}.proj.bias",
                hf_patterns=["encoder.mid_block.attentions.{i}.proj.bias"],
            ),
            # ========== Encoder output ==========
            WeightTarget(
                mlx_path="encoder.norm_out.weight",
                hf_patterns=["encoder.norm_out.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="encoder.conv_out.conv3d.weight",
                hf_patterns=["encoder.conv_out.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="encoder.conv_out.conv3d.bias",
                hf_patterns=["encoder.conv_out.bias"],
            ),
            # ========== Decoder conv_in ==========
            WeightTarget(
                mlx_path="decoder.conv_in.conv3d.weight",
                hf_patterns=["decoder.conv_in.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="decoder.conv_in.conv3d.bias",
                hf_patterns=["decoder.conv_in.bias"],
            ),
            # ========== Decoder mid_block ==========
            # Mid block resnets
            WeightTarget(
                mlx_path="decoder.mid_block.resnets.{i}.norm1.weight",
                hf_patterns=["decoder.mid_block.resnets.{i}.norm1.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="decoder.mid_block.resnets.{i}.conv1.conv3d.weight",
                hf_patterns=["decoder.mid_block.resnets.{i}.conv1.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="decoder.mid_block.resnets.{i}.conv1.conv3d.bias",
                hf_patterns=["decoder.mid_block.resnets.{i}.conv1.bias"],
            ),
            WeightTarget(
                mlx_path="decoder.mid_block.resnets.{i}.norm2.weight",
                hf_patterns=["decoder.mid_block.resnets.{i}.norm2.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="decoder.mid_block.resnets.{i}.conv2.conv3d.weight",
                hf_patterns=["decoder.mid_block.resnets.{i}.conv2.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="decoder.mid_block.resnets.{i}.conv2.conv3d.bias",
                hf_patterns=["decoder.mid_block.resnets.{i}.conv2.bias"],
            ),
            # Mid block attention
            WeightTarget(
                mlx_path="decoder.mid_block.attentions.{i}.norm.weight",
                hf_patterns=["decoder.mid_block.attentions.{i}.norm.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="decoder.mid_block.attentions.{i}.to_qkv.weight",
                hf_patterns=["decoder.mid_block.attentions.{i}.to_qkv.weight"],
                transform=transpose_conv2d_weight,
            ),
            WeightTarget(
                mlx_path="decoder.mid_block.attentions.{i}.to_qkv.bias",
                hf_patterns=["decoder.mid_block.attentions.{i}.to_qkv.bias"],
            ),
            WeightTarget(
                mlx_path="decoder.mid_block.attentions.{i}.proj.weight",
                hf_patterns=["decoder.mid_block.attentions.{i}.proj.weight"],
                transform=transpose_conv2d_weight,
            ),
            WeightTarget(
                mlx_path="decoder.mid_block.attentions.{i}.proj.bias",
                hf_patterns=["decoder.mid_block.attentions.{i}.proj.bias"],
            ),
            # ========== Decoder up_blocks ==========
            # Up blocks resnets
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.resnets.{res}.norm1.weight",
                hf_patterns=["decoder.up_blocks.{block}.resnets.{res}.norm1.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.resnets.{res}.conv1.conv3d.weight",
                hf_patterns=["decoder.up_blocks.{block}.resnets.{res}.conv1.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.resnets.{res}.conv1.conv3d.bias",
                hf_patterns=["decoder.up_blocks.{block}.resnets.{res}.conv1.bias"],
            ),
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.resnets.{res}.norm2.weight",
                hf_patterns=["decoder.up_blocks.{block}.resnets.{res}.norm2.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.resnets.{res}.conv2.conv3d.weight",
                hf_patterns=["decoder.up_blocks.{block}.resnets.{res}.conv2.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.resnets.{res}.conv2.conv3d.bias",
                hf_patterns=["decoder.up_blocks.{block}.resnets.{res}.conv2.bias"],
            ),
            # Up blocks resnets - conv_shortcut (optional, when in_dim != out_dim)
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.resnets.{res}.conv_shortcut.conv3d.weight",
                hf_patterns=["decoder.up_blocks.{block}.resnets.{res}.conv_shortcut.weight"],
                transform=transpose_conv3d_weight,
                required=False,  # Optional - only exists when dimensions differ
            ),
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.resnets.{res}.conv_shortcut.conv3d.bias",
                hf_patterns=["decoder.up_blocks.{block}.resnets.{res}.conv_shortcut.bias"],
                required=False,  # Optional - only exists when dimensions differ
            ),
            # Up blocks upsamplers - time_conv (blocks 0, 1 only)
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.upsampler.time_conv.conv3d.weight",
                hf_patterns=["decoder.up_blocks.{block}.upsampler.time_conv.weight"],
                transform=transpose_conv3d_weight,
                required=False,  # Only exists for blocks 0, 1
            ),
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.upsampler.time_conv.conv3d.bias",
                hf_patterns=["decoder.up_blocks.{block}.upsampler.time_conv.bias"],
                required=False,  # Only exists for blocks 0, 1
            ),
            # Up blocks upsamplers - resample_conv (blocks 0, 1, 2)
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.upsampler.resample_conv.weight",
                hf_patterns=["decoder.up_blocks.{block}.upsampler.resample.1.weight"],
                transform=transpose_conv2d_weight,
                required=False,  # Only exists for blocks 0, 1, 2
            ),
            WeightTarget(
                mlx_path="decoder.up_blocks.{block}.upsampler.resample_conv.bias",
                hf_patterns=["decoder.up_blocks.{block}.upsampler.resample.1.bias"],
                required=False,  # Only exists for blocks 0, 1, 2
            ),
            # ========== Decoder output ==========
            WeightTarget(
                mlx_path="decoder.norm_out.weight",
                hf_patterns=["decoder.norm_out.gamma"],
                transform=reshape_gamma_to_1d,
            ),
            WeightTarget(
                mlx_path="decoder.conv_out.conv3d.weight",
                hf_patterns=["decoder.conv_out.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="decoder.conv_out.conv3d.bias",
                hf_patterns=["decoder.conv_out.bias"],
            ),
            # ========== Quant conv ==========
            WeightTarget(
                mlx_path="quant_conv.conv3d.weight",
                hf_patterns=["quant_conv.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="quant_conv.conv3d.bias",
                hf_patterns=["quant_conv.bias"],
            ),
            # ========== Post quant conv ==========
            WeightTarget(
                mlx_path="post_quant_conv.conv3d.weight",
                hf_patterns=["post_quant_conv.weight"],
                transform=transpose_conv3d_weight,
            ),
            WeightTarget(
                mlx_path="post_quant_conv.conv3d.bias",
                hf_patterns=["post_quant_conv.bias"],
            ),
        ]
