"""Attention utilities for FLUX.2.

Reuses the FLUX.1 attention utilities as the core attention mechanism is the same.
"""

from mflux.models.flux.model.flux_transformer.common.attention_utils import AttentionUtils

__all__ = ["AttentionUtils"]
