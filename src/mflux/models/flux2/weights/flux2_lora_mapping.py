from mflux.models.common.lora.mapping.lora_mapping import LoRAMapping, LoRATarget
from mflux.models.common.lora.mapping.lora_transforms import LoraTransforms


class Flux2LoRAMapping(LoRAMapping):
    @staticmethod
    def get_mapping() -> list[LoRATarget]:
        targets: list[LoRATarget] = []

        targets.extend(Flux2LoRAMapping._get_global_targets())

        # Standard (explicit) naming conventions
        targets.extend(Flux2LoRAMapping._get_double_stream_block_targets())
        targets.extend(Flux2LoRAMapping._get_single_stream_block_targets())

        # BFL / ComfyUI-style naming conventions (qkv split, etc.)
        targets.extend(Flux2LoRAMapping._get_bfl_double_stream_block_targets())
        targets.extend(Flux2LoRAMapping._get_bfl_single_stream_block_targets())

        return targets

    @staticmethod
    def _get_global_targets() -> list[LoRATarget]:
        return [
            LoRATarget(
                model_path="x_embedder",
                possible_up_patterns=[
                    "x_embedder.lora_B.weight",
                    "x_embedder.lora_up.weight",
                    "transformer.x_embedder.lora_B.weight",
                    "transformer.x_embedder.lora_up.weight",
                    "diffusion_model.x_embedder.lora_B.weight",
                    "diffusion_model.x_embedder.lora_up.weight",
                    "base_model.model.img_in.lora_B.weight",
                    "base_model.model.img_in.lora_up.weight",
                ],
                possible_down_patterns=[
                    "x_embedder.lora_A.weight",
                    "x_embedder.lora_down.weight",
                    "transformer.x_embedder.lora_A.weight",
                    "transformer.x_embedder.lora_down.weight",
                    "diffusion_model.x_embedder.lora_A.weight",
                    "diffusion_model.x_embedder.lora_down.weight",
                    "base_model.model.img_in.lora_A.weight",
                    "base_model.model.img_in.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "x_embedder.alpha",
                    "transformer.x_embedder.alpha",
                    "diffusion_model.x_embedder.alpha",
                    "base_model.model.img_in.alpha",
                ],
            ),
            LoRATarget(
                model_path="context_embedder",
                possible_up_patterns=[
                    "context_embedder.lora_B.weight",
                    "context_embedder.lora_up.weight",
                    "transformer.context_embedder.lora_B.weight",
                    "transformer.context_embedder.lora_up.weight",
                    "diffusion_model.context_embedder.lora_B.weight",
                    "diffusion_model.context_embedder.lora_up.weight",
                    "base_model.model.txt_in.lora_B.weight",
                    "base_model.model.txt_in.lora_up.weight",
                ],
                possible_down_patterns=[
                    "context_embedder.lora_A.weight",
                    "context_embedder.lora_down.weight",
                    "transformer.context_embedder.lora_A.weight",
                    "transformer.context_embedder.lora_down.weight",
                    "diffusion_model.context_embedder.lora_A.weight",
                    "diffusion_model.context_embedder.lora_down.weight",
                    "base_model.model.txt_in.lora_A.weight",
                    "base_model.model.txt_in.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "context_embedder.alpha",
                    "transformer.context_embedder.alpha",
                    "diffusion_model.context_embedder.alpha",
                    "base_model.model.txt_in.alpha",
                ],
            ),
            LoRATarget(
                model_path="time_guidance_embed.linear_1",
                possible_up_patterns=[
                    "time_guidance_embed.linear_1.lora_B.weight",
                    "time_guidance_embed.linear_1.lora_up.weight",
                    "transformer.time_guidance_embed.linear_1.lora_B.weight",
                    "transformer.time_guidance_embed.linear_1.lora_up.weight",
                    "diffusion_model.time_guidance_embed.linear_1.lora_B.weight",
                    "diffusion_model.time_guidance_embed.linear_1.lora_up.weight",
                    "base_model.model.time_in.in_layer.lora_B.weight",
                    "base_model.model.time_in.in_layer.lora_up.weight",
                ],
                possible_down_patterns=[
                    "time_guidance_embed.linear_1.lora_A.weight",
                    "time_guidance_embed.linear_1.lora_down.weight",
                    "transformer.time_guidance_embed.linear_1.lora_A.weight",
                    "transformer.time_guidance_embed.linear_1.lora_down.weight",
                    "diffusion_model.time_guidance_embed.linear_1.lora_A.weight",
                    "diffusion_model.time_guidance_embed.linear_1.lora_down.weight",
                    "base_model.model.time_in.in_layer.lora_A.weight",
                    "base_model.model.time_in.in_layer.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "time_guidance_embed.linear_1.alpha",
                    "transformer.time_guidance_embed.linear_1.alpha",
                    "diffusion_model.time_guidance_embed.linear_1.alpha",
                    "base_model.model.time_in.in_layer.alpha",
                ],
            ),
            LoRATarget(
                model_path="time_guidance_embed.linear_2",
                possible_up_patterns=[
                    "time_guidance_embed.linear_2.lora_B.weight",
                    "time_guidance_embed.linear_2.lora_up.weight",
                    "transformer.time_guidance_embed.linear_2.lora_B.weight",
                    "transformer.time_guidance_embed.linear_2.lora_up.weight",
                    "diffusion_model.time_guidance_embed.linear_2.lora_B.weight",
                    "diffusion_model.time_guidance_embed.linear_2.lora_up.weight",
                    "base_model.model.time_in.out_layer.lora_B.weight",
                    "base_model.model.time_in.out_layer.lora_up.weight",
                ],
                possible_down_patterns=[
                    "time_guidance_embed.linear_2.lora_A.weight",
                    "time_guidance_embed.linear_2.lora_down.weight",
                    "transformer.time_guidance_embed.linear_2.lora_A.weight",
                    "transformer.time_guidance_embed.linear_2.lora_down.weight",
                    "diffusion_model.time_guidance_embed.linear_2.lora_A.weight",
                    "diffusion_model.time_guidance_embed.linear_2.lora_down.weight",
                    "base_model.model.time_in.out_layer.lora_A.weight",
                    "base_model.model.time_in.out_layer.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "time_guidance_embed.linear_2.alpha",
                    "transformer.time_guidance_embed.linear_2.alpha",
                    "diffusion_model.time_guidance_embed.linear_2.alpha",
                    "base_model.model.time_in.out_layer.alpha",
                ],
            ),
            # Optional guidance embeddings (only present if guidance_embeds=True)
            LoRATarget(
                model_path="time_guidance_embed.guidance_linear_1",
                possible_up_patterns=[
                    "time_guidance_embed.guidance_linear_1.lora_B.weight",
                    "time_guidance_embed.guidance_linear_1.lora_up.weight",
                    "transformer.time_guidance_embed.guidance_linear_1.lora_B.weight",
                    "transformer.time_guidance_embed.guidance_linear_1.lora_up.weight",
                    "diffusion_model.time_guidance_embed.guidance_linear_1.lora_B.weight",
                    "diffusion_model.time_guidance_embed.guidance_linear_1.lora_up.weight",
                ],
                possible_down_patterns=[
                    "time_guidance_embed.guidance_linear_1.lora_A.weight",
                    "time_guidance_embed.guidance_linear_1.lora_down.weight",
                    "transformer.time_guidance_embed.guidance_linear_1.lora_A.weight",
                    "transformer.time_guidance_embed.guidance_linear_1.lora_down.weight",
                    "diffusion_model.time_guidance_embed.guidance_linear_1.lora_A.weight",
                    "diffusion_model.time_guidance_embed.guidance_linear_1.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "time_guidance_embed.guidance_linear_1.alpha",
                    "transformer.time_guidance_embed.guidance_linear_1.alpha",
                    "diffusion_model.time_guidance_embed.guidance_linear_1.alpha",
                ],
            ),
            LoRATarget(
                model_path="time_guidance_embed.guidance_linear_2",
                possible_up_patterns=[
                    "time_guidance_embed.guidance_linear_2.lora_B.weight",
                    "time_guidance_embed.guidance_linear_2.lora_up.weight",
                    "transformer.time_guidance_embed.guidance_linear_2.lora_B.weight",
                    "transformer.time_guidance_embed.guidance_linear_2.lora_up.weight",
                    "diffusion_model.time_guidance_embed.guidance_linear_2.lora_B.weight",
                    "diffusion_model.time_guidance_embed.guidance_linear_2.lora_up.weight",
                ],
                possible_down_patterns=[
                    "time_guidance_embed.guidance_linear_2.lora_A.weight",
                    "time_guidance_embed.guidance_linear_2.lora_down.weight",
                    "transformer.time_guidance_embed.guidance_linear_2.lora_A.weight",
                    "transformer.time_guidance_embed.guidance_linear_2.lora_down.weight",
                    "diffusion_model.time_guidance_embed.guidance_linear_2.lora_A.weight",
                    "diffusion_model.time_guidance_embed.guidance_linear_2.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "time_guidance_embed.guidance_linear_2.alpha",
                    "transformer.time_guidance_embed.guidance_linear_2.alpha",
                    "diffusion_model.time_guidance_embed.guidance_linear_2.alpha",
                ],
            ),
            LoRATarget(
                model_path="double_stream_modulation_img.linear",
                possible_up_patterns=[
                    "double_stream_modulation_img.linear.lora_B.weight",
                    "double_stream_modulation_img.linear.lora_up.weight",
                    "transformer.double_stream_modulation_img.linear.lora_B.weight",
                    "transformer.double_stream_modulation_img.linear.lora_up.weight",
                    "diffusion_model.double_stream_modulation_img.linear.lora_B.weight",
                    "diffusion_model.double_stream_modulation_img.linear.lora_up.weight",
                    "base_model.model.double_stream_modulation_img.lin.lora_B.weight",
                    "base_model.model.double_stream_modulation_img.lin.lora_up.weight",
                ],
                possible_down_patterns=[
                    "double_stream_modulation_img.linear.lora_A.weight",
                    "double_stream_modulation_img.linear.lora_down.weight",
                    "transformer.double_stream_modulation_img.linear.lora_A.weight",
                    "transformer.double_stream_modulation_img.linear.lora_down.weight",
                    "diffusion_model.double_stream_modulation_img.linear.lora_A.weight",
                    "diffusion_model.double_stream_modulation_img.linear.lora_down.weight",
                    "base_model.model.double_stream_modulation_img.lin.lora_A.weight",
                    "base_model.model.double_stream_modulation_img.lin.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "double_stream_modulation_img.linear.alpha",
                    "transformer.double_stream_modulation_img.linear.alpha",
                    "diffusion_model.double_stream_modulation_img.linear.alpha",
                    "base_model.model.double_stream_modulation_img.lin.alpha",
                ],
            ),
            LoRATarget(
                model_path="double_stream_modulation_txt.linear",
                possible_up_patterns=[
                    "double_stream_modulation_txt.linear.lora_B.weight",
                    "double_stream_modulation_txt.linear.lora_up.weight",
                    "transformer.double_stream_modulation_txt.linear.lora_B.weight",
                    "transformer.double_stream_modulation_txt.linear.lora_up.weight",
                    "diffusion_model.double_stream_modulation_txt.linear.lora_B.weight",
                    "diffusion_model.double_stream_modulation_txt.linear.lora_up.weight",
                    "base_model.model.double_stream_modulation_txt.lin.lora_B.weight",
                    "base_model.model.double_stream_modulation_txt.lin.lora_up.weight",
                ],
                possible_down_patterns=[
                    "double_stream_modulation_txt.linear.lora_A.weight",
                    "double_stream_modulation_txt.linear.lora_down.weight",
                    "transformer.double_stream_modulation_txt.linear.lora_A.weight",
                    "transformer.double_stream_modulation_txt.linear.lora_down.weight",
                    "diffusion_model.double_stream_modulation_txt.linear.lora_A.weight",
                    "diffusion_model.double_stream_modulation_txt.linear.lora_down.weight",
                    "base_model.model.double_stream_modulation_txt.lin.lora_A.weight",
                    "base_model.model.double_stream_modulation_txt.lin.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "double_stream_modulation_txt.linear.alpha",
                    "transformer.double_stream_modulation_txt.linear.alpha",
                    "diffusion_model.double_stream_modulation_txt.linear.alpha",
                    "base_model.model.double_stream_modulation_txt.lin.alpha",
                ],
            ),
            LoRATarget(
                model_path="single_stream_modulation.linear",
                possible_up_patterns=[
                    "single_stream_modulation.linear.lora_B.weight",
                    "single_stream_modulation.linear.lora_up.weight",
                    "transformer.single_stream_modulation.linear.lora_B.weight",
                    "transformer.single_stream_modulation.linear.lora_up.weight",
                    "diffusion_model.single_stream_modulation.linear.lora_B.weight",
                    "diffusion_model.single_stream_modulation.linear.lora_up.weight",
                    "base_model.model.single_stream_modulation.lin.lora_B.weight",
                    "base_model.model.single_stream_modulation.lin.lora_up.weight",
                ],
                possible_down_patterns=[
                    "single_stream_modulation.linear.lora_A.weight",
                    "single_stream_modulation.linear.lora_down.weight",
                    "transformer.single_stream_modulation.linear.lora_A.weight",
                    "transformer.single_stream_modulation.linear.lora_down.weight",
                    "diffusion_model.single_stream_modulation.linear.lora_A.weight",
                    "diffusion_model.single_stream_modulation.linear.lora_down.weight",
                    "base_model.model.single_stream_modulation.lin.lora_A.weight",
                    "base_model.model.single_stream_modulation.lin.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "single_stream_modulation.linear.alpha",
                    "transformer.single_stream_modulation.linear.alpha",
                    "diffusion_model.single_stream_modulation.linear.alpha",
                    "base_model.model.single_stream_modulation.lin.alpha",
                ],
            ),
            LoRATarget(
                model_path="norm_out.linear",
                possible_up_patterns=[
                    "norm_out.linear.lora_B.weight",
                    "norm_out.linear.lora_up.weight",
                    "transformer.norm_out.linear.lora_B.weight",
                    "transformer.norm_out.linear.lora_up.weight",
                    "diffusion_model.norm_out.linear.lora_B.weight",
                    "diffusion_model.norm_out.linear.lora_up.weight",
                ],
                possible_down_patterns=[
                    "norm_out.linear.lora_A.weight",
                    "norm_out.linear.lora_down.weight",
                    "transformer.norm_out.linear.lora_A.weight",
                    "transformer.norm_out.linear.lora_down.weight",
                    "diffusion_model.norm_out.linear.lora_A.weight",
                    "diffusion_model.norm_out.linear.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "norm_out.linear.alpha",
                    "transformer.norm_out.linear.alpha",
                    "diffusion_model.norm_out.linear.alpha",
                ],
            ),
            LoRATarget(
                model_path="proj_out",
                possible_up_patterns=[
                    "proj_out.lora_B.weight",
                    "proj_out.lora_up.weight",
                    "transformer.proj_out.lora_B.weight",
                    "transformer.proj_out.lora_up.weight",
                    "diffusion_model.proj_out.lora_B.weight",
                    "diffusion_model.proj_out.lora_up.weight",
                    "base_model.model.final_layer.linear.lora_B.weight",
                    "base_model.model.final_layer.linear.lora_up.weight",
                ],
                possible_down_patterns=[
                    "proj_out.lora_A.weight",
                    "proj_out.lora_down.weight",
                    "transformer.proj_out.lora_A.weight",
                    "transformer.proj_out.lora_down.weight",
                    "diffusion_model.proj_out.lora_A.weight",
                    "diffusion_model.proj_out.lora_down.weight",
                    "base_model.model.final_layer.linear.lora_A.weight",
                    "base_model.model.final_layer.linear.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "proj_out.alpha",
                    "transformer.proj_out.alpha",
                    "diffusion_model.proj_out.alpha",
                    "base_model.model.final_layer.linear.alpha",
                ],
            ),
        ]

    @staticmethod
    def _get_double_stream_block_targets() -> list[LoRATarget]:
        return [
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.to_q",
                possible_up_patterns=[
                    "transformer_blocks.{block}.attn.to_q.lora_B.weight",
                    "transformer_blocks.{block}.attn.to_q.lora_up.weight",
                    "transformer.transformer_blocks.{block}.attn.to_q.lora_B.weight",
                    "transformer.transformer_blocks.{block}.attn.to_q.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_q.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_q.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_q.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.attn.to_q.lora_A.weight",
                    "transformer_blocks.{block}.attn.to_q.lora_down.weight",
                    "transformer.transformer_blocks.{block}.attn.to_q.lora_A.weight",
                    "transformer.transformer_blocks.{block}.attn.to_q.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_q.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_q.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_q.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.attn.to_q.alpha",
                    "lora_unet_transformer_blocks_{block}_attn_to_q.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.to_k",
                possible_up_patterns=[
                    "transformer_blocks.{block}.attn.to_k.lora_B.weight",
                    "transformer_blocks.{block}.attn.to_k.lora_up.weight",
                    "transformer.transformer_blocks.{block}.attn.to_k.lora_B.weight",
                    "transformer.transformer_blocks.{block}.attn.to_k.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_k.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_k.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_k.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.attn.to_k.lora_A.weight",
                    "transformer_blocks.{block}.attn.to_k.lora_down.weight",
                    "transformer.transformer_blocks.{block}.attn.to_k.lora_A.weight",
                    "transformer.transformer_blocks.{block}.attn.to_k.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_k.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_k.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_k.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.attn.to_k.alpha",
                    "lora_unet_transformer_blocks_{block}_attn_to_k.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.to_v",
                possible_up_patterns=[
                    "transformer_blocks.{block}.attn.to_v.lora_B.weight",
                    "transformer_blocks.{block}.attn.to_v.lora_up.weight",
                    "transformer.transformer_blocks.{block}.attn.to_v.lora_B.weight",
                    "transformer.transformer_blocks.{block}.attn.to_v.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_v.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_v.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_v.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.attn.to_v.lora_A.weight",
                    "transformer_blocks.{block}.attn.to_v.lora_down.weight",
                    "transformer.transformer_blocks.{block}.attn.to_v.lora_A.weight",
                    "transformer.transformer_blocks.{block}.attn.to_v.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_v.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_v.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_v.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.attn.to_v.alpha",
                    "lora_unet_transformer_blocks_{block}_attn_to_v.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.to_out",
                possible_up_patterns=[
                    "transformer_blocks.{block}.attn.to_out.lora_B.weight",
                    "transformer_blocks.{block}.attn.to_out.lora_up.weight",
                    "transformer_blocks.{block}.attn.to_out.0.lora_B.weight",  # HF-style
                    "transformer_blocks.{block}.attn.to_out.0.lora_up.weight",  # HF-style
                    "transformer.transformer_blocks.{block}.attn.to_out.lora_B.weight",
                    "transformer.transformer_blocks.{block}.attn.to_out.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_out.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_out.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_out.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_out_0.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.attn.to_out.lora_A.weight",
                    "transformer_blocks.{block}.attn.to_out.lora_down.weight",
                    "transformer_blocks.{block}.attn.to_out.0.lora_A.weight",  # HF-style
                    "transformer_blocks.{block}.attn.to_out.0.lora_down.weight",  # HF-style
                    "transformer.transformer_blocks.{block}.attn.to_out.lora_A.weight",
                    "transformer.transformer_blocks.{block}.attn.to_out.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_out.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_out.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_out.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_out_0.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.attn.to_out.alpha",
                    "transformer_blocks.{block}.attn.to_out.0.alpha",
                    "lora_unet_transformer_blocks_{block}_attn_to_out.alpha",
                    "lora_unet_transformer_blocks_{block}_attn_to_out_0.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.add_q_proj",
                possible_up_patterns=[
                    "transformer_blocks.{block}.attn.add_q_proj.lora_B.weight",
                    "transformer_blocks.{block}.attn.add_q_proj.lora_up.weight",
                    "transformer.transformer_blocks.{block}.attn.add_q_proj.lora_B.weight",
                    "transformer.transformer_blocks.{block}.attn.add_q_proj.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_q_proj.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_q_proj.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_attn_add_q_proj.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.attn.add_q_proj.lora_A.weight",
                    "transformer_blocks.{block}.attn.add_q_proj.lora_down.weight",
                    "transformer.transformer_blocks.{block}.attn.add_q_proj.lora_A.weight",
                    "transformer.transformer_blocks.{block}.attn.add_q_proj.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_q_proj.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_q_proj.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_attn_add_q_proj.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.attn.add_q_proj.alpha",
                    "lora_unet_transformer_blocks_{block}_attn_add_q_proj.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.add_k_proj",
                possible_up_patterns=[
                    "transformer_blocks.{block}.attn.add_k_proj.lora_B.weight",
                    "transformer_blocks.{block}.attn.add_k_proj.lora_up.weight",
                    "transformer.transformer_blocks.{block}.attn.add_k_proj.lora_B.weight",
                    "transformer.transformer_blocks.{block}.attn.add_k_proj.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_k_proj.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_k_proj.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_attn_add_k_proj.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.attn.add_k_proj.lora_A.weight",
                    "transformer_blocks.{block}.attn.add_k_proj.lora_down.weight",
                    "transformer.transformer_blocks.{block}.attn.add_k_proj.lora_A.weight",
                    "transformer.transformer_blocks.{block}.attn.add_k_proj.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_k_proj.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_k_proj.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_attn_add_k_proj.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.attn.add_k_proj.alpha",
                    "lora_unet_transformer_blocks_{block}_attn_add_k_proj.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.add_v_proj",
                possible_up_patterns=[
                    "transformer_blocks.{block}.attn.add_v_proj.lora_B.weight",
                    "transformer_blocks.{block}.attn.add_v_proj.lora_up.weight",
                    "transformer.transformer_blocks.{block}.attn.add_v_proj.lora_B.weight",
                    "transformer.transformer_blocks.{block}.attn.add_v_proj.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_v_proj.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_v_proj.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_attn_add_v_proj.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.attn.add_v_proj.lora_A.weight",
                    "transformer_blocks.{block}.attn.add_v_proj.lora_down.weight",
                    "transformer.transformer_blocks.{block}.attn.add_v_proj.lora_A.weight",
                    "transformer.transformer_blocks.{block}.attn.add_v_proj.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_v_proj.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.add_v_proj.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_attn_add_v_proj.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.attn.add_v_proj.alpha",
                    "lora_unet_transformer_blocks_{block}_attn_add_v_proj.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.to_add_out",
                possible_up_patterns=[
                    "transformer_blocks.{block}.attn.to_add_out.lora_B.weight",
                    "transformer_blocks.{block}.attn.to_add_out.lora_up.weight",
                    "transformer.transformer_blocks.{block}.attn.to_add_out.lora_B.weight",
                    "transformer.transformer_blocks.{block}.attn.to_add_out.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_add_out.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_add_out.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_add_out.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.attn.to_add_out.lora_A.weight",
                    "transformer_blocks.{block}.attn.to_add_out.lora_down.weight",
                    "transformer.transformer_blocks.{block}.attn.to_add_out.lora_A.weight",
                    "transformer.transformer_blocks.{block}.attn.to_add_out.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_add_out.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.attn.to_add_out.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_attn_to_add_out.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.attn.to_add_out.alpha",
                    "lora_unet_transformer_blocks_{block}_attn_to_add_out.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.ff.linear_in",
                possible_up_patterns=[
                    "transformer_blocks.{block}.ff.linear_in.lora_B.weight",
                    "transformer_blocks.{block}.ff.linear_in.lora_up.weight",
                    "transformer.transformer_blocks.{block}.ff.linear_in.lora_B.weight",
                    "transformer.transformer_blocks.{block}.ff.linear_in.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.ff.linear_in.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.ff.linear_in.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_ff_linear_in.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.ff.linear_in.lora_A.weight",
                    "transformer_blocks.{block}.ff.linear_in.lora_down.weight",
                    "transformer.transformer_blocks.{block}.ff.linear_in.lora_A.weight",
                    "transformer.transformer_blocks.{block}.ff.linear_in.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.ff.linear_in.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.ff.linear_in.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_ff_linear_in.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.ff.linear_in.alpha",
                    "lora_unet_transformer_blocks_{block}_ff_linear_in.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.ff.linear_out",
                possible_up_patterns=[
                    "transformer_blocks.{block}.ff.linear_out.lora_B.weight",
                    "transformer_blocks.{block}.ff.linear_out.lora_up.weight",
                    "transformer.transformer_blocks.{block}.ff.linear_out.lora_B.weight",
                    "transformer.transformer_blocks.{block}.ff.linear_out.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.ff.linear_out.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.ff.linear_out.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_ff_linear_out.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.ff.linear_out.lora_A.weight",
                    "transformer_blocks.{block}.ff.linear_out.lora_down.weight",
                    "transformer.transformer_blocks.{block}.ff.linear_out.lora_A.weight",
                    "transformer.transformer_blocks.{block}.ff.linear_out.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.ff.linear_out.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.ff.linear_out.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_ff_linear_out.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.ff.linear_out.alpha",
                    "lora_unet_transformer_blocks_{block}_ff_linear_out.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.ff_context.linear_in",
                possible_up_patterns=[
                    "transformer_blocks.{block}.ff_context.linear_in.lora_B.weight",
                    "transformer_blocks.{block}.ff_context.linear_in.lora_up.weight",
                    "transformer.transformer_blocks.{block}.ff_context.linear_in.lora_B.weight",
                    "transformer.transformer_blocks.{block}.ff_context.linear_in.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.ff_context.linear_in.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.ff_context.linear_in.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_ff_context_linear_in.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.ff_context.linear_in.lora_A.weight",
                    "transformer_blocks.{block}.ff_context.linear_in.lora_down.weight",
                    "transformer.transformer_blocks.{block}.ff_context.linear_in.lora_A.weight",
                    "transformer.transformer_blocks.{block}.ff_context.linear_in.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.ff_context.linear_in.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.ff_context.linear_in.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_ff_context_linear_in.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.ff_context.linear_in.alpha",
                    "lora_unet_transformer_blocks_{block}_ff_context_linear_in.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.ff_context.linear_out",
                possible_up_patterns=[
                    "transformer_blocks.{block}.ff_context.linear_out.lora_B.weight",
                    "transformer_blocks.{block}.ff_context.linear_out.lora_up.weight",
                    "transformer.transformer_blocks.{block}.ff_context.linear_out.lora_B.weight",
                    "transformer.transformer_blocks.{block}.ff_context.linear_out.lora_up.weight",
                    "diffusion_model.transformer_blocks.{block}.ff_context.linear_out.lora_B.weight",
                    "diffusion_model.transformer_blocks.{block}.ff_context.linear_out.lora_up.weight",
                    "lora_unet_transformer_blocks_{block}_ff_context_linear_out.lora_up.weight",
                ],
                possible_down_patterns=[
                    "transformer_blocks.{block}.ff_context.linear_out.lora_A.weight",
                    "transformer_blocks.{block}.ff_context.linear_out.lora_down.weight",
                    "transformer.transformer_blocks.{block}.ff_context.linear_out.lora_A.weight",
                    "transformer.transformer_blocks.{block}.ff_context.linear_out.lora_down.weight",
                    "diffusion_model.transformer_blocks.{block}.ff_context.linear_out.lora_A.weight",
                    "diffusion_model.transformer_blocks.{block}.ff_context.linear_out.lora_down.weight",
                    "lora_unet_transformer_blocks_{block}_ff_context_linear_out.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "transformer_blocks.{block}.ff_context.linear_out.alpha",
                    "lora_unet_transformer_blocks_{block}_ff_context_linear_out.alpha",
                ],
            ),
        ]

    @staticmethod
    def _get_single_stream_block_targets() -> list[LoRATarget]:
        return [
            LoRATarget(
                model_path="single_transformer_blocks.{block}.attn.to_qkv_mlp_proj",
                possible_up_patterns=[
                    "single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_B.weight",
                    "single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_up.weight",
                    "transformer.single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_B.weight",
                    "transformer.single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_up.weight",
                    "diffusion_model.single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_B.weight",
                    "diffusion_model.single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_up.weight",
                    "lora_unet_single_transformer_blocks_{block}_attn_to_qkv_mlp_proj.lora_up.weight",
                ],
                possible_down_patterns=[
                    "single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_A.weight",
                    "single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_down.weight",
                    "transformer.single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_A.weight",
                    "transformer.single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_down.weight",
                    "diffusion_model.single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_A.weight",
                    "diffusion_model.single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.lora_down.weight",
                    "lora_unet_single_transformer_blocks_{block}_attn_to_qkv_mlp_proj.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "single_transformer_blocks.{block}.attn.to_qkv_mlp_proj.alpha",
                    "lora_unet_single_transformer_blocks_{block}_attn_to_qkv_mlp_proj.alpha",
                ],
            ),
            LoRATarget(
                model_path="single_transformer_blocks.{block}.attn.to_out",
                possible_up_patterns=[
                    "single_transformer_blocks.{block}.attn.to_out.lora_B.weight",
                    "single_transformer_blocks.{block}.attn.to_out.lora_up.weight",
                    "transformer.single_transformer_blocks.{block}.attn.to_out.lora_B.weight",
                    "transformer.single_transformer_blocks.{block}.attn.to_out.lora_up.weight",
                    "diffusion_model.single_transformer_blocks.{block}.attn.to_out.lora_B.weight",
                    "diffusion_model.single_transformer_blocks.{block}.attn.to_out.lora_up.weight",
                    "lora_unet_single_transformer_blocks_{block}_attn_to_out.lora_up.weight",
                ],
                possible_down_patterns=[
                    "single_transformer_blocks.{block}.attn.to_out.lora_A.weight",
                    "single_transformer_blocks.{block}.attn.to_out.lora_down.weight",
                    "transformer.single_transformer_blocks.{block}.attn.to_out.lora_A.weight",
                    "transformer.single_transformer_blocks.{block}.attn.to_out.lora_down.weight",
                    "diffusion_model.single_transformer_blocks.{block}.attn.to_out.lora_A.weight",
                    "diffusion_model.single_transformer_blocks.{block}.attn.to_out.lora_down.weight",
                    "lora_unet_single_transformer_blocks_{block}_attn_to_out.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "single_transformer_blocks.{block}.attn.to_out.alpha",
                    "lora_unet_single_transformer_blocks_{block}_attn_to_out.alpha",
                ],
            ),
        ]

    @staticmethod
    def _get_bfl_double_stream_block_targets() -> list[LoRATarget]:
        return [
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.to_q",
                possible_up_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_qkv.lora_up.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_B.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_up.weight",
                ],
                possible_down_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_qkv.lora_down.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_A.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_qkv.alpha",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.alpha",
                ],
                up_transform=LoraTransforms.split_q_up,
                down_transform=LoraTransforms.split_q_down,
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.to_k",
                possible_up_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_qkv.lora_up.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_B.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_up.weight",
                ],
                possible_down_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_qkv.lora_down.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_A.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_qkv.alpha",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.alpha",
                ],
                up_transform=LoraTransforms.split_k_up,
                down_transform=LoraTransforms.split_k_down,
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.to_v",
                possible_up_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_qkv.lora_up.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_B.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_up.weight",
                ],
                possible_down_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_qkv.lora_down.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_A.weight",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_qkv.alpha",
                    "base_model.model.double_blocks.{block}.img_attn.qkv.alpha",
                ],
                up_transform=LoraTransforms.split_v_up,
                down_transform=LoraTransforms.split_v_down,
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.to_out",
                possible_up_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_proj.lora_up.weight",
                    "base_model.model.double_blocks.{block}.img_attn.proj.lora_B.weight",
                    "base_model.model.double_blocks.{block}.img_attn.proj.lora_up.weight",
                ],
                possible_down_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_proj.lora_down.weight",
                    "base_model.model.double_blocks.{block}.img_attn.proj.lora_A.weight",
                    "base_model.model.double_blocks.{block}.img_attn.proj.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "lora_unet_double_blocks_{block}_img_attn_proj.alpha",
                    "base_model.model.double_blocks.{block}.img_attn.proj.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.add_q_proj",
                possible_up_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_qkv.lora_up.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_B.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_up.weight",
                ],
                possible_down_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_qkv.lora_down.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_A.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_qkv.alpha",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.alpha",
                ],
                up_transform=LoraTransforms.split_q_up,
                down_transform=LoraTransforms.split_q_down,
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.add_k_proj",
                possible_up_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_qkv.lora_up.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_B.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_up.weight",
                ],
                possible_down_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_qkv.lora_down.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_A.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_qkv.alpha",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.alpha",
                ],
                up_transform=LoraTransforms.split_k_up,
                down_transform=LoraTransforms.split_k_down,
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.add_v_proj",
                possible_up_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_qkv.lora_up.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_B.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_up.weight",
                ],
                possible_down_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_qkv.lora_down.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_A.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_qkv.alpha",
                    "base_model.model.double_blocks.{block}.txt_attn.qkv.alpha",
                ],
                up_transform=LoraTransforms.split_v_up,
                down_transform=LoraTransforms.split_v_down,
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.attn.to_add_out",
                possible_up_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_proj.lora_up.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.proj.lora_B.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.proj.lora_up.weight",
                ],
                possible_down_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_proj.lora_down.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.proj.lora_A.weight",
                    "base_model.model.double_blocks.{block}.txt_attn.proj.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "lora_unet_double_blocks_{block}_txt_attn_proj.alpha",
                    "base_model.model.double_blocks.{block}.txt_attn.proj.alpha",
                ],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.ff.linear_in",
                possible_up_patterns=["lora_unet_double_blocks_{block}_img_mlp_0.lora_up.weight"],
                possible_down_patterns=["lora_unet_double_blocks_{block}_img_mlp_0.lora_down.weight"],
                possible_alpha_patterns=["lora_unet_double_blocks_{block}_img_mlp_0.alpha"],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.ff.linear_out",
                possible_up_patterns=["lora_unet_double_blocks_{block}_img_mlp_2.lora_up.weight"],
                possible_down_patterns=["lora_unet_double_blocks_{block}_img_mlp_2.lora_down.weight"],
                possible_alpha_patterns=["lora_unet_double_blocks_{block}_img_mlp_2.alpha"],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.ff_context.linear_in",
                possible_up_patterns=["lora_unet_double_blocks_{block}_txt_mlp_0.lora_up.weight"],
                possible_down_patterns=["lora_unet_double_blocks_{block}_txt_mlp_0.lora_down.weight"],
                possible_alpha_patterns=["lora_unet_double_blocks_{block}_txt_mlp_0.alpha"],
            ),
            LoRATarget(
                model_path="transformer_blocks.{block}.ff_context.linear_out",
                possible_up_patterns=["lora_unet_double_blocks_{block}_txt_mlp_2.lora_up.weight"],
                possible_down_patterns=["lora_unet_double_blocks_{block}_txt_mlp_2.lora_down.weight"],
                possible_alpha_patterns=["lora_unet_double_blocks_{block}_txt_mlp_2.alpha"],
            ),
        ]

    @staticmethod
    def _get_bfl_single_stream_block_targets() -> list[LoRATarget]:
        # Flux2 single blocks fuse qkv+mlp into one projection, matching BFL "linear1".
        return [
            LoRATarget(
                model_path="single_transformer_blocks.{block}.attn.to_qkv_mlp_proj",
                possible_up_patterns=[
                    "lora_unet_single_blocks_{block}_linear1.lora_up.weight",
                    "base_model.model.single_blocks.{block}.linear1.lora_B.weight",
                    "base_model.model.single_blocks.{block}.linear1.lora_up.weight",
                ],
                possible_down_patterns=[
                    "lora_unet_single_blocks_{block}_linear1.lora_down.weight",
                    "base_model.model.single_blocks.{block}.linear1.lora_A.weight",
                    "base_model.model.single_blocks.{block}.linear1.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "lora_unet_single_blocks_{block}_linear1.alpha",
                    "base_model.model.single_blocks.{block}.linear1.alpha",
                ],
            ),
            LoRATarget(
                model_path="single_transformer_blocks.{block}.attn.to_out",
                possible_up_patterns=[
                    "lora_unet_single_blocks_{block}_linear2.lora_up.weight",
                    "base_model.model.single_blocks.{block}.linear2.lora_B.weight",
                    "base_model.model.single_blocks.{block}.linear2.lora_up.weight",
                ],
                possible_down_patterns=[
                    "lora_unet_single_blocks_{block}_linear2.lora_down.weight",
                    "base_model.model.single_blocks.{block}.linear2.lora_A.weight",
                    "base_model.model.single_blocks.{block}.linear2.lora_down.weight",
                ],
                possible_alpha_patterns=[
                    "lora_unet_single_blocks_{block}_linear2.alpha",
                    "base_model.model.single_blocks.{block}.linear2.alpha",
                ],
            ),
        ]
