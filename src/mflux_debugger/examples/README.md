# Lightweight ML Debugger for PyTorch and MLX

Interactive debugging system optimized for ML workloads. Designed for AI agents to step through code, inspect tensors, and verify model implementations in real-time.

**Version: 0.2.2** - Now with async execution, trace recording, crash resilience, clean slate debugging workflow, and real-world model porting examples!

---

## Table of Contents

1. [Quick Start](#quick-start)
2. [Installing Dependencies in Editable Mode](#installing-dependencies-in-editable-mode)
3. [Core Philosophy](#core-philosophy)
4. [REST API Usage](#rest-api-usage-recommended)
5. [Complete Debugging Workflows](#complete-debugging-workflows)
6. [MLX Lazy Evaluation](#mlx-lazy-evaluation-essential)
7. [Model Porting Guide](#model-porting-guide)
8. [Live Debugging Lessons Learned](#live-debugging-lessons-learned)

---

## Quick Start

### Start the REST API Server

```bash
cd /path/to/mflux
uvicorn mflux_debugger.fastapi_server:app --host 127.0.0.1 --port 8000
```

### Basic Workflow

```bash
# 1. Start debugging session
curl -X POST http://localhost:8000/debug/start \
  -H "Content-Type: application/json" \
  -d '{"script_path": "src/mflux_debugger/examples/debug_mflux.py"}'

# 2. Set breakpoint
curl -X POST http://localhost:8000/debug/breakpoint \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/file.py", "line": 100}'

# 3. Start execution (non-blocking for ML workloads!)
curl -X POST http://localhost:8000/debug/continue_async

# 4. Poll for breakpoint
curl http://localhost:8000/debug/status

# 5. When paused, inspect variables
curl http://localhost:8000/debug/location
curl -X POST http://localhost:8000/debug/evaluate \
  -H "Content-Type: application/json" \
  -d '{"expression": "latents.shape"}'

# 6. Terminate (auto-saves trace)
curl -X POST http://localhost:8000/debug/terminate
```

**Interactive API docs:** http://localhost:8000/docs

---

## Installing Dependencies in Editable Mode

### CRITICAL: When debugging code from external libraries (like `diffusers`), you **must** install them in editable mode. Otherwise, breakpoints won't work because Python loads the installed package from `.venv/lib/python3.XX/site-packages/` instead of your local clone.

### Why This Matters:
- When you set a breakpoint at `/Users/you/Desktop/diffusers/src/diffusers/file.py:123`
- But Python loads `/Users/you/mflux/.venv/lib/.../diffusers/file.py`
- The paths don't match â†’ breakpoint never hits! ğŸš¨

### Solution:

```bash
# 1. Clone the library you want to debug
cd ~/Desktop
git clone https://github.com/huggingface/diffusers.git

# 2. Install it in editable mode in your mflux environment
cd ~/Desktop/mflux
source .venv/bin/activate
uv pip install -e ~/Desktop/diffusers

# 3. Also install mflux itself in editable mode for debugging
uv pip install -e .

# 4. Verify editable installation
uv pip list | grep diffusers
# Should show: diffusers  X.X.X  /Users/you/Desktop/diffusers

# 5. Now breakpoints will work!
# Set breakpoint at: /Users/you/Desktop/diffusers/src/diffusers/pipelines/flux/pipeline_flux.py:856
```

### Common Debugging Setup:

```bash
# Install commonly debugged libraries in editable mode
uv pip install -e ~/Desktop/diffusers     # For PyTorch reference implementations
uv pip install -e ~/Desktop/transformers  # For text encoders
uv pip install -e ~/Desktop/mflux         # For MFLUX itself

# Verify all are editable
uv pip list | grep -E "(diffusers|transformers|mflux)"
```

### Troubleshooting Path Issues:

**If breakpoints still don't hit:**

1. **Check which version is loaded:**
```python
# In your debug session, evaluate:
import diffusers
print(diffusers.__file__)
# Should point to: /Users/you/Desktop/diffusers/...
# NOT: .venv/lib/python3.XX/site-packages/diffusers/...
```

2. **Use absolute paths for breakpoints:**
```bash
# âœ… GOOD - Absolute path
curl -X POST http://localhost:8000/debug/breakpoint \
  -d '{"file_path": "/Users/you/Desktop/diffusers/src/diffusers/pipelines/flux/pipeline_flux.py", "line": 856}'

# âŒ BAD - Relative path
curl -X POST http://localhost:8000/debug/breakpoint \
  -d '{"file_path": "../diffusers/src/diffusers/pipelines/flux/pipeline_flux.py", "line": 856}'
```

3. **Restart your debug server after installing in editable mode:**
```bash
# Kill the old server
pkill -f "uvicorn mflux_debugger"

# Start fresh
uvicorn mflux_debugger.fastapi_server:app --host 127.0.0.1 --port 8000
```

---

## Core Philosophy

### âš ï¸ Interactive Debugging Over Scripts

**CRITICAL:** When investigating models or comparing implementations, **USE THE DEBUGGER**, not temporary scripts!

**Why:**
- âœ… Inspects actual production code
- âœ… Can't check the wrong thing
- âœ… No maintenance burden
- âœ… Git handles versioning
- âœ… Trustworthy results

**Problems with scripts:**
- âŒ Can compare different computation points
- âŒ Hard to maintain, accumulate over time
- âŒ Can be written incorrectly
- âŒ Create dual implementations that drift

**Only write scripts if explicitly requested.**

### Example: Comparing Implementations (Good Approach)

âŒ **DON'T:**
```python
# Comparison script - might compare wrong things
def compare_encoders():
    mlx_enc = load_mlx()
    torch_enc = load_torch()
    # Easy to load wrong weights or compare wrong points
```

âœ… **DO:**
```
# Debug Session 1 (PyTorch reference)
Set breakpoint at pipeline_flux.py line 828
Inspect prompt_embeds.shape  # [1, 512, 4096]

# Debug Session 2 (MLX implementation)  
Set breakpoint at flux.py line 72
Inspect prompt_embeds.shape  # Verify matches reference
```

Traces are automatically saved for offline comparison!

---

## REST API Usage (Recommended)

### Why REST API over MCP?

âœ… **More stable** - HTTP is battle-tested  
âœ… **Language agnostic** - Any tool can use it  
âœ… **Better for automation** - Easy to script  
âœ… **Interactive docs** - Swagger UI built-in

### Key Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/debug/start` | POST | Start session |
| `/debug/breakpoint` | POST | Set breakpoint |
| `/debug/breakpoint/remove` | POST | Remove breakpoint |
| `/debug/breakpoints` | GET | List all breakpoints |
| `/debug/breakpoints/clear` | POST | Clear all breakpoints |
| `/debug/continue_async` | POST | **Non-blocking continue** |
| `/debug/step/over` | POST | Step over line |
| `/debug/step/into` | POST | Step into function |
| `/debug/step/out` | POST | Step out of function |
| `/debug/evaluate` | POST | Evaluate expression |
| `/debug/location` | GET | Current location |
| `/debug/status` | GET | Check if paused |
| `/debug/variables` | GET | List variables |
| `/debug/terminate` | POST | End session + save trace |

### Async Execution (Essential for ML!)

**Problem:** Model loading takes 30-60 seconds â†’ HTTP timeout!

**Solution:** Async execution with polling

#### Basic Polling (Infinite Loop)
```bash
# Start execution (returns immediately)
curl -X POST http://localhost:8000/debug/continue_async

# Poll for status (every 2-5 seconds)
while true; do
  STATUS=$(curl -s http://localhost:8000/debug/status | jq -r '.data.state')
  if [ "$STATUS" = "paused" ]; then
    echo "Hit breakpoint!"
    break
  fi
  sleep 3
done

# Now inspect
curl http://localhost:8000/debug/location
```

#### Smart Polling with Progress Indicator (Recommended)
```bash
# Start execution
curl -X POST http://localhost:8000/debug/continue_async

# Poll with countdown and progress indicator
# Adjust max iterations based on expected load time:
# - 10 iterations = ~20 seconds (for quick operations)
# - 20 iterations = ~60 seconds (for model loading)
# - 30 iterations = ~60 seconds (for slower models)
echo "Waiting for breakpoint..." && for i in {1..20}; do
  sleep 3
  STATUS=$(curl -s http://localhost:8000/debug/status | grep -o '"state":"[^"]*"' | cut -d'"' -f4)
  echo "[$i/20] Status: $STATUS"
  if [ "$STATUS" = "paused" ]; then
    echo "âœ… Hit breakpoint!"
    break
  fi
done

# Now inspect
curl http://localhost:8000/debug/location
```

**Why this is better:**
- âœ… Shows progress with `[N/20]` counter
- âœ… Exits early when breakpoint hits (doesn't wait full duration)
- âœ… Uses `grep` instead of `jq` (fewer dependencies)
- âœ… Visual feedback with âœ… emoji
- âœ… Clear timeout indication if max iterations reached

---

## Complete Debugging Workflows

### Workflow 1: Debugging Diffusers (PyTorch)

```bash
# Start server
uvicorn mflux_debugger.fastapi_server:app --host 127.0.0.1 --port 8000 &

# Start session
curl -X POST http://localhost:8000/debug/start \
  -H "Content-Type: application/json" \
  -d '{"script_path": "src/mflux_debugger/examples/debug_diffusers.py"}'

# Set strategic breakpoints
curl -X POST http://localhost:8000/debug/breakpoint \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/diffusers/src/diffusers/pipelines/flux/pipeline_flux.py", "line": 828}'  # Prompt encoding

curl -X POST http://localhost:8000/debug/breakpoint \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/diffusers/src/diffusers/pipelines/flux/pipeline_flux.py", "line": 856}'  # Latent creation

# Start async execution
curl -X POST http://localhost:8000/debug/continue_async

# Poll for model loading with progress indicator (~45 seconds)
echo "Waiting for model to load..." && for i in {1..20}; do
  sleep 3
  STATUS=$(curl -s http://localhost:8000/debug/status | grep -o '"state":"[^"]*"' | cut -d'"' -f4)
  echo "[$i/20] Status: $STATUS"
  if [ "$STATUS" = "paused" ]; then echo "âœ… Hit breakpoint!"; break; fi
done

# Inspect text embeddings
curl -X POST http://localhost:8000/debug/evaluate \
  -H "Content-Type: application/json" \
  -d '{"expression": "prompt_embeds.shape"}'
# Returns: [1, 512, 4096]

# Continue to next breakpoint
curl -X POST http://localhost:8000/debug/continue_async

# Poll for next breakpoint (faster this time)
for i in {1..5}; do
  sleep 1
  STATUS=$(curl -s http://localhost:8000/debug/status | grep -o '"state":"[^"]*"' | cut -d'"' -f4)
  echo "[$i/5] Status: $STATUS"
  if [ "$STATUS" = "paused" ]; then echo "âœ… Hit breakpoint!"; break; fi
done

# Inspect latents
curl -X POST http://localhost:8000/debug/evaluate \
  -H "Content-Type: application/json" \
  -d '{"expression": "latents.shape"}'
# Returns: [1, 256, 64]

# Terminate (saves trace automatically)
curl -X POST http://localhost:8000/debug/terminate
# Check: mflux_debugger/traces/debug_diffusers_YYYYMMDD_HHMMSS.json
```

### Workflow 2: Debugging MLX Implementation

```bash
# Start session
curl -X POST http://localhost:8000/debug/start \
  -H "Content-Type: application/json" \
  -d '{"script_path": "src/mflux_debugger/examples/debug_mflux.py"}'

# Set breakpoint (after model loads)
curl -X POST http://localhost:8000/debug/breakpoint \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/mflux/src/mflux/models/flux/variants/txt2img/flux.py", "line": 144}'  # VAE decode

# Start async execution
curl -X POST http://localhost:8000/debug/continue_async

# Poll for breakpoint with progress indicator (model loads for ~60 seconds)
echo "Waiting for MLX model to load..." && for i in {1..30}; do
  sleep 2
  STATUS=$(curl -s http://localhost:8000/debug/status | grep -o '"state":"[^"]*"' | cut -d'"' -f4)
  echo "[$i/30] Status: $STATUS"
  if [ "$STATUS" = "paused" ]; then
    echo "âœ… Hit breakpoint!"
    break
  fi
done

# Inspect latents before VAE
curl -X POST http://localhost:8000/debug/evaluate \
  -H "Content-Type: application/json" \
  -d '{"expression": "latents.shape"}'
# Returns: [1, 16, 32, 32]

# Compare with PyTorch trace to verify match!

# Terminate
curl -X POST http://localhost:8000/debug/terminate
```

### Workflow 3: Managing Breakpoints in Loops

**Problem:** Breakpoint inside loop hits 4+ times (annoying!)

**Solution:** Remove breakpoint after first hit

```bash
# Set breakpoint in loop
curl -X POST http://localhost:8000/debug/breakpoint \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/file.py", "line": 944}'

# Continue until hit
curl -X POST http://localhost:8000/debug/continue_async

# ... wait for pause ...

# Inspect once
curl -X POST http://localhost:8000/debug/evaluate \
  -H "Content-Type: application/json" \
  -d '{"expression": "i"}'

# Remove breakpoint so we don't hit it again
curl -X POST http://localhost:8000/debug/breakpoint/remove \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/file.py", "line": 944}'

# Continue to next breakpoint (skips remaining loop iterations)
curl -X POST http://localhost:8000/debug/continue_async
```

---

## MLX Lazy Evaluation (Essential!)

### The Challenge

**MLX uses lazy evaluation** - computations are deferred until `mx.eval()`:

```python
# This doesn't compute anything!
latents = mx.random.normal(shape=[1, 256, 64])

# Computation happens here:
mx.eval(latents)  # NOW it executes
```

**Impact on debugging:**
- âŒ Breakpoint BEFORE `mx.eval()` â†’ tensor has no values
- âŒ `.mean()`, `.shape` â†’ also lazy
- âŒ Debugger shows nothing or hangs

### Solution: Debugger Auto-Handles This!

**Good news:** The lightweight debugger **automatically handles MLX lazy evaluation**!

- âœ… Selectively traces only user code
- âœ… Skips ML library internals
- âœ… Evaluates tensors automatically when inspecting
- âœ… No manual `mx.eval()` needed for debugging

### Best Breakpoint Locations for MLX

âœ… **GOOD** - After natural evaluation points:
```python
for t in time_steps:
    latents = self.transformer(...)
    latents = self.scheduler.step(...)
    mx.eval(latents)  # Already here for progress!
    # âœ… Set breakpoint here - tensor is evaluated
```

âŒ **BAD** - Before evaluation:
```python
latents = mx.random.normal(...)
# âŒ Breakpoint here - lazy, not computed yet
mx.eval(latents)
# âœ… Breakpoint here - now computed
```

---

## Model Porting Guide

### Working Backwards Strategy (Proven Successful)

**Problem:** Entire pipeline is hard to debug at once.

**Solution:** Implement and verify components **backwards**, from output to input.

```
Traditional (hard):
Text â†’ Latent â†’ Transformer â†’ VAE â†’ Image
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ implement everything â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                   then debug

Backwards (easy):
Text â† Latent â† Transformer â† VAE â† Image
                              â†‘
                         Start here!
```

**Implementation order:**

1. **VAE Decoder** (outputs images - easy to verify visually)
2. **Transformer last block** (verify numerically)
3. **Work backwards through transformer blocks**
4. **Text encoder**
5. **Latent creation**
6. **End-to-end check**

**Why this works:**
- Isolates each component
- Visual verification early (VAE produces images)
- Removes non-determinism by using saved inputs
- Builds confidence incrementally

### Verification Framework

**Tolerance levels:**
- `error < 1e-5` â†’ âœ… Clearly correct
- `1e-5 < error < 0.1` â†’ âš ï¸ Check visual output
- `error > 0.1` â†’ âŒ Clearly broken

**Don't reject implementations in the gray area without visual inspection!**

### Small Verified Chunks

âœ… **Verify each component thoroughly before moving on**  
âœ… **Don't accumulate uncertainty**  
âœ… **Use Git to revert if needed**  
âŒ **Don't doubt previously verified components**

---

## Live Debugging Lessons Learned

### Session 1: Path Normalization Bug (FIXED!)

**Problem:** Breakpoints not hitting in Diffusers pipeline

**Root cause:**
- Breakpoints stored with absolute paths: `/Users/.../diffusers/file.py`
- Frame paths were relative: `../diffusers/file.py`
- Mismatch â†’ breakpoint never matched!

**Fix:** Normalize all paths to absolute before comparison

```python
# In check_breakpoint():
filename = str(Path(frame.f_code.co_filename).resolve())
```

**Result:** âœ… Breakpoints hit reliably!

### Session 2: Crash Resilience (IMPLEMENTED!)

**Problem:** Server crashed during debugging â†’ lost all trace data

**Root cause:** Traces only saved on explicit `terminate()` call

**Fix:** Incremental auto-save after every step

**Features added:**
- âœ… Auto-save after each breakpoint/evaluation
- âœ… Emergency save on unexpected exit (`atexit` handler)
- âœ… Timestamped trace files (never overwrite)
- âœ… Automatic session cleanup on restart

**Result:** Never lose debugging progress, even on crash!

### Session 3: Loop Breakpoints (NOW MANAGEABLE!)

**Problem:** Breakpoint in denoising loop hit 4x (once per step)

**Solution:** Added breakpoint management API

**New endpoints:**
- `POST /debug/breakpoint/remove` - Remove specific breakpoint
- `GET /debug/breakpoints` - List all breakpoints
- `POST /debug/breakpoints/clear` - Clear all at once

**Workflow:**
```bash
# Hit breakpoint once
curl -X POST http://localhost:8000/debug/continue_async

# Inspect
curl -X POST http://localhost:8000/debug/evaluate -d '{"expression": "i"}'

# Remove so we don't hit it again
curl -X POST http://localhost:8000/debug/breakpoint/remove \
  -d '{"file_path": "...", "line": 944}'

# Continue to next breakpoint (skips remaining iterations)
curl -X POST http://localhost:8000/debug/continue_async
```

### Session 4: Tensor Shape Verification

**Finding:** PyTorch and MLX implementations have **identical tensor shapes**!

| Stage | Tensor | Shape | Match |
|-------|--------|-------|-------|
| Text encoding (T5) | prompt_embeds | `[1, 512, 4096]` | âœ… |
| Text encoding (CLIP) | pooled_embeds | `[1, 768]` | âœ… |
| Initial latents | latents | `[1, 256, 64]` | âœ… |
| Unpacked latents | latents | `[1, 16, 32, 32]` | âœ… |

**This proves the architecture is correctly ported!**

### Session 5: Trace Recording

**Feature:** Automatic trace recording for offline analysis

**What's captured:**
- All breakpoints set
- All execution steps with location, code context, call stack
- All evaluations with results
- Timestamps for performance analysis

**Trace format:**
```json
{
  "session_id": "20251024_133020",
  "script_path": "/path/to/debug_mflux.py",
  "start_time": "2025-10-24T13:30:20...",
  "end_time": "2025-10-24T13:32:02...",
  "breakpoints": [
    {"file": "...", "line": 144, "condition": null}
  ],
  "steps": [
    {
      "step": 1,
      "step_type": "continue_async",
      "location": {"file": "...", "line": 144, "function": "generate_image"},
      "code_context": {"before": [...], "current": [...], "after": [...]},
      "call_stack": [...],
      "variables": {...}
    },
    {
      "step": 2,
      "step_type": "evaluation",
      "expression": "latents.shape",
      "result": "[1, 16, 32, 32]"
    }
  ],
  "total_steps": 6
}
```

**Location:** `mflux_debugger/traces/debug_NAME_TIMESTAMP.json`

### Session 6: Editable Installation Discovery (CRITICAL!)

**Problem:** Breakpoints not hitting in external library code (diffusers, transformers)

**Root cause:** Python loading package from `.venv/lib/python3.XX/site-packages/` instead of local clone

**Symptoms:**
- Breakpoint set at `/Users/you/Desktop/diffusers/src/diffusers/file.py:856`
- But Python loads from `/Users/you/mflux/.venv/lib/python3.11/site-packages/diffusers/...`
- Path mismatch â†’ debugger never pauses!

**The Fix:** Install external libraries in editable mode:
```bash
uv pip install -e ~/Desktop/diffusers
uv pip install -e ~/Desktop/transformers
uv pip install -e ~/Desktop/mflux  # Don't forget mflux itself!
```

**How to verify:**
```python
# In debug session, evaluate:
import diffusers
print(diffusers.__file__)
# âœ… Should show: /Users/you/Desktop/diffusers/...
# âŒ BAD: .venv/lib/.../site-packages/diffusers/...
```

**Alternative check:**
```bash
uv pip list | grep diffusers
# âœ… Good: diffusers  0.30.0  /Users/you/Desktop/diffusers
# âŒ Bad: diffusers  0.30.0
```

**Result:** This single change makes debugging external libraries possible! Without editable mode, you can only debug your own code effectively.

**Important:** Always restart the debug server after switching to editable mode:
```bash
pkill -f "uvicorn mflux_debugger"
uvicorn mflux_debugger.fastapi_server:app --host 127.0.0.1 --port 8000
```

### Session 7: Virtual Environment Best Practices

**Lesson:** Keep debugging environments clean and consistent

**Best practices:**
1. **One virtual environment per project being debugged:**
   ```bash
   # mflux environment (for debugging mflux)
   cd ~/Desktop/mflux
   python3 -m venv .venv
   source .venv/bin/activate
   uv pip install -e .
   uv pip install -e ~/Desktop/diffusers  # Reference implementation
   ```

2. **Always verify which environment is active:**
   ```bash
   which python
   # Should show: /Users/you/Desktop/mflux/.venv/bin/python
   ```

3. **Document your setup in a script:**
   ```bash
   # ~/Desktop/mflux/setup_debug_env.sh
   #!/bin/bash
   python3 -m venv .venv
   source .venv/bin/activate
   uv pip install -e .
   uv pip install -e ~/Desktop/diffusers
   uv pip install -e ~/Desktop/transformers
   uv pip install fastapi uvicorn
   ```

4. **Reset environment if things get weird:**
   ```bash
   cd ~/Desktop/mflux
   rm -rf .venv
   ./setup_debug_env.sh
   ```

**Common mistakes:**
- âŒ Installing packages in global Python
- âŒ Forgetting to activate venv before debugging
- âŒ Installing regular (non-editable) packages after editable ones
- âŒ Having multiple conflicting versions in different locations

### Session 8: Real-World Model Port Verification (QWEN IMAGE)

**Task:** Verify PyTorchâ†’MLX port by comparing tensor values at key stages

**The typical request:**
> "I would like you to step through the main chunks of the model (text encoder, initial latent, transformer, VAE) for both diffusers and MLX. Compare tensor values to ensure correct porting."

**Challenge:** Random seed produces different initial latents across frameworks

#### Solution: Minimal Code Intervention (No Helper Scripts!)

**Step 1: Save reference latents from diffusers**

```python
# In /path/to/diffusers/src/diffusers/pipelines/qwenimage/pipeline_qwenimage.py
# After line: latents = self.prepare_latents(...)

# TEMP: Save latents for debugging
np.save("/Users/you/Desktop/initial_latents_diffusers.npy", latents.cpu().float().numpy())
import sys; sys.exit(0)  # Exit immediately after saving
```

Run once to save, then comment out the exit:
```python
# TEMP: Save latents for debugging
np.save("/Users/you/Desktop/initial_latents_diffusers.npy", latents.cpu().float().numpy())
# import sys; sys.exit(0)  # (commented out for full debugging)
```

**Step 2: Load saved latents in MLX**

```python
# In /path/to/mflux/src/mflux/latent_creator/latent_creator.py
import numpy as np
from pathlib import Path

@staticmethod
def create(seed: int, height: int, width: int) -> mx.array:
    # TEMP: Load latents from diffusers for comparison
    latents_path = Path("/Users/you/Desktop/initial_latents_diffusers.npy")
    if latents_path.exists():
        latents_np = np.load(latents_path)
        return mx.array(latents_np)
    
    # Original random generation
    return mx.random.normal(...)
```

**Step 3: Debug both implementations interactively**

```bash
# Debug Diffusers
curl -X POST http://localhost:8000/debug/start \
  -d '{"script_path": "src/mflux_debugger/examples/debug_diffusers.py"}'

# Set breakpoints at key locations
curl -X POST http://localhost:8000/debug/breakpoint \
  -d '{"file_path": "/path/to/pipeline.py", "line": 623}'  # After text encoding
curl -X POST http://localhost:8000/debug/breakpoint \
  -d '{"file_path": "/path/to/pipeline.py", "line": 639}'  # After latents
curl -X POST http://localhost:8000/debug/breakpoint \
  -d '{"file_path": "/path/to/pipeline.py", "line": 765}'  # Before VAE

# Run and inspect at each breakpoint
curl -X POST http://localhost:8000/debug/continue_async

# Poll for breakpoints and evaluate tensors
echo "Waiting..." && for i in {1..20}; do
  sleep 3
  STATUS=$(curl -s http://localhost:8000/debug/status | grep -o '"state":"[^"]*"' | cut -d'"' -f4)
  echo "[$i/20] Status: $STATUS"
  if [ "$STATUS" = "paused" ]; then break; fi
done

# Inspect
curl -X POST http://localhost:8000/debug/evaluate \
  -d '{"expression": "prompt_embeds.shape"}'
curl -X POST http://localhost:8000/debug/evaluate \
  -d '{"expression": "(latents.mean().item(), latents.std().item())"}'

# Continue to next breakpoint...
curl -X POST http://localhost:8000/debug/continue_async
```

Repeat for MLX implementation.

#### Results from Real Session

| Component | Diffusers | MLX | Match? |
|-----------|-----------|-----|--------|
| **Text Encoder** | [1, 14, 3584]<br>mean=-0.104, std=5.09 | [1, 14, 3584]<br>mean=-0.101, std=5.06 | âœ… |
| **Initial Latents** | [1, 256, 64]<br>*(saved to disk)* | [1, 256, 64]<br>*(loaded from disk)* | âœ… |
| **Latents before VAE** | [1, 16, 1, 32, 32]<br>mean=-0.052, std=1.07 | [1, 16, 32, 32]<br>mean=-0.028, std=0.56 | âš ï¸ |

**Finding:** Text encoder matches perfectly. Denoising loop has divergence (transformer/scheduler issue).

#### Key Principles Demonstrated

1. **âœ… Minimal code changes:** 2 lines to save, 3 lines to load
2. **âœ… No helper scripts:** All debugging via API calls in terminal
3. **âœ… No workspace clutter:** Temporary changes in source files (easily reverted)
4. **âœ… Interactive inspection:** Live evaluation at each breakpoint
5. **âœ… Comment after use:** Once saved, comment out exit line for full debugging
6. **âœ… Direct comparison:** Same input latents ensure fair comparison

**Why this approach is better than writing scripts:**
- âŒ Scripts need maintenance and accumulate over time
- âŒ Scripts can compare wrong things or be written incorrectly
- âœ… Interactive debugging inspects actual production code
- âœ… Git makes it easy to revert temporary changes
- âœ… Follows "Core Philosophy" of the debugger

**Lesson:** When asked to compare implementations, use **minimal inline changes + interactive debugging**, not helper scripts!

---

## Tips for AI Agents

### Do's âœ…

- **Always install libraries in editable mode** (`uv pip install -e`) when debugging external code
- **Test with simple breakpoints first** before debugging deep library code
- **Kill and restart server** if breakpoints mysteriously don't hit
- Use async execution (`/debug/continue_async`) for ML workloads
- **Use smart polling with progress indicators** instead of blind sleep:
  ```bash
  # âœ… GOOD - Shows progress, exits early
  for i in {1..20}; do
    sleep 3
    STATUS=$(curl -s http://localhost:8000/debug/status | grep -o '"state":"[^"]*"' | cut -d'"' -f4)
    echo "[$i/20] Status: $STATUS"
    if [ "$STATUS" = "paused" ]; then echo "âœ… Hit breakpoint!"; break; fi
  done
  
  # âŒ BAD - Blind wait, no feedback
  sleep 60 && curl http://localhost:8000/debug/location
  ```
- Remove breakpoints after first hit in loops
- Start with PyTorch (reference) before debugging MLX
- Check trace files for offline analysis
- Use `/debug/evaluate` for quick inspections
- Verify which package version is loaded (`import X; print(X.__file__)`)
- Use absolute paths for breakpoints
- **For model comparison:** Make minimal inline changes (save/load data) rather than writing comparison scripts
- Comment out temporary debugging lines after use (e.g., `sys.exit()` after saving data)

### Don'ts âŒ

- Don't use blocking `/debug/continue` for model loading (timeout!)
- Don't set breakpoints in ML library code (slow!)
- Don't leave breakpoints in tight loops
- Don't compare different computation points across implementations
- **Don't write temporary comparison scripts** - use interactive debugging instead
- **Don't immediately debug library code** - start with entry point breakpoints first
- Don't create helper files that clutter the workspace - make minimal inline changes

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transport Layer (choose one)       â”‚
â”‚  - FastAPI REST API (recommended)   â”‚
â”‚  - MCP Server (Cursor integration)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DebuggerService (business logic)   â”‚
â”‚  - Session management                â”‚
â”‚  - Breakpoint management             â”‚
â”‚  - Variable formatting               â”‚
â”‚  - Rich context capture              â”‚
â”‚  - Async execution                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LightweightDebugger (core)         â”‚
â”‚  - Selective tracing (user code)    â”‚
â”‚  - Library skipping (torch, mlx)    â”‚
â”‚  - Thread-safe state caching        â”‚
â”‚  - MLX auto-evaluation               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TraceRecorder (persistence)        â”‚
â”‚  - Incremental auto-save             â”‚
â”‚  - Emergency save on crash           â”‚
â”‚  - Rich context capture              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Performance

**Overhead comparison:**

| Operation | Debugpy | Lightweight | Speedup |
|-----------|---------|-------------|---------|
| Startup | 2-3s | ~0.1s | 20-30x |
| Model loading | 60s+ (often hangs) | Normal speed | âˆ |
| Step over | ~0.5s | ~0.01s | 50x |
| Variable inspection | ~0.2s | ~0.001s | 200x |

**Key advantage:** Doesn't slow down model loading because it skips ML library internals!

---

## Requirements

```bash
# Core
uv pip install fastapi uvicorn

# For PyTorch debugging (reference)
uv pip install torch diffusers

# For MLX debugging
# (MLX is pre-installed on mflux project)
```

---

## Troubleshooting

### Breakpoint not hitting?

**Check:**
1. **Is the library installed in editable mode?** This is the #1 cause!
   ```bash
   uv pip list | grep library_name
   # Should show a path like: /Users/you/Desktop/library_name
   ```
   If not, install in editable mode:
   ```bash
   uv pip install -e ~/Desktop/library_name
   ```
   Then restart the debug server.

2. **Is the path absolute?** Use absolute paths for `file_path`
   ```bash
   # âœ… Good
   /Users/you/Desktop/diffusers/src/diffusers/pipelines/flux/pipeline_flux.py
   
   # âŒ Bad
   ../diffusers/src/diffusers/pipelines/flux/pipeline_flux.py
   ```

3. **Is Python loading the right version?**
   ```bash
   curl -X POST http://localhost:8000/debug/evaluate \
     -d '{"expression": "import diffusers; diffusers.__file__"}'
   # Should show your editable install path
   ```

4. **Is the script running?** Check `/debug/status`

5. **Is the line executable?** Set breakpoint on actual code, not comments/blank lines

6. **Clean slate approach:** If breakpoints mysteriously don't hit, try this workflow:
   ```bash
   # 1. Kill any stale debug servers
   pkill -f "uvicorn mflux_debugger"
   
   # 2. Start fresh server
   uvicorn mflux_debugger.fastapi_server:app --host 127.0.0.1 --port 8000 &
   sleep 2
   
   # 3. Start new session
   curl -X POST http://localhost:8000/debug/start \
     -H "Content-Type: application/json" \
     -d '{"script_path": "your_script.py"}'
   
   # 4. Test with a SIMPLE breakpoint first (entry point of your script)
   curl -X POST http://localhost:8000/debug/breakpoint \
     -H "Content-Type: application/json" \
     -d '{"file_path": "/absolute/path/to/your_script.py", "line": 6}'
   
   # 5. Run and verify it hits with smart polling
   curl -X POST http://localhost:8000/debug/continue_async
   
   # Poll with progress (adjust iterations based on your script)
   for i in {1..5}; do
     sleep 1
     STATUS=$(curl -s http://localhost:8000/debug/status | grep -o '"state":"[^"]*"' | cut -d'"' -f4)
     echo "[$i/5] Status: $STATUS"
     if [ "$STATUS" = "paused" ]; then echo "âœ… Hit breakpoint!"; break; fi
   done
   
   curl http://localhost:8000/debug/location
   
   # 6. If that works, clear it and set your real breakpoints
   curl -X POST http://localhost:8000/debug/breakpoints/clear
   # Now set breakpoints in library code...
   ```

### Best Practice: Test Simple First, Then Complex

**âŒ DON'T start with:**
```bash
# Immediately setting breakpoints deep in library code
curl -X POST .../debug/breakpoint -d '{"file_path": ".../diffusers/.../pipeline.py", "line": 856}'
curl -X POST .../debug/continue_async
# ğŸ˜± Doesn't hit, now what's wrong?
```

**âœ… DO start with:**
```bash
# First: Set breakpoint at entry point of YOUR script
curl -X POST .../debug/breakpoint -d '{"file_path": "/path/to/your_script.py", "line": 6}'
curl -X POST .../debug/continue_async
# âœ… Hits? Great! Debugger is working. Clear and set real breakpoints.
# âŒ Doesn't hit? Fix the fundamentals (editable install, server restart, etc)
```

**Why this matters:**
- âœ… Verifies debugger trace is working
- âœ… Confirms paths are resolving correctly
- âœ… Ensures clean session state
- âœ… Builds confidence before debugging complex library code

**Rule of thumb:** If unsure, start with breakpoints in your own script, then work your way into libraries.

### "Still running" after 60 seconds?

**This is normal for model loading!** Use async execution with smart polling:
```bash
curl -X POST http://localhost:8000/debug/continue_async

# Poll with progress indicator (adjust iterations for your model)
echo "Waiting for model..." && for i in {1..30}; do
  sleep 2
  STATUS=$(curl -s http://localhost:8000/debug/status | grep -o '"state":"[^"]*"' | cut -d'"' -f4)
  echo "[$i/30] Status: $STATUS"
  if [ "$STATUS" = "paused" ]; then echo "âœ… Hit breakpoint!"; break; fi
done
```

### Variables show `null`?

**For MLX:** Breakpoint might be before evaluation. Move to after `mx.eval()` call.

### Lost trace after crash?

**Good news:** Traces are auto-saved after every step! Check:
```bash
ls -lt mflux_debugger/traces/*.json | head -1
```

---

## Credits

Developed through live debugging sessions comparing FLUX.1 and Qwen-Image PyTorch (Diffusers) and MLX implementations.

**Version 0.2.2** includes lessons learned from:
- **Real-world model port verification** (QWEN-Image PyTorchâ†’MLX)
- **Minimal code intervention philosophy** (no helper scripts)
- **Smart polling patterns** (progress indicators for async operations)
- **Editable installation discovery** (breakpoint path matching)
- **Clean slate debugging workflow** (test simple breakpoints first)
- Path normalization debugging
- Crash resilience testing
- Loop breakpoint management
- Async execution for ML workloads
- Tensor shape verification across frameworks
- Virtual environment best practices
- Stale server state troubleshooting

---

**Happy Debugging! ğŸ› â†’ âœ…**
