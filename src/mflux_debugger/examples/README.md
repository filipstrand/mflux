# Lightweight ML Debugger for PyTorch and MLX

Interactive debugging system optimized for ML workloads. Designed for AI agents to step through code, inspect tensors, and verify model implementations in real-time.

**Version: 0.2.0** - Now with async execution, trace recording, and crash resilience!

---

## Table of Contents

1. [Quick Start](#quick-start)
2. [Core Philosophy](#core-philosophy)
3. [REST API Usage](#rest-api-usage-recommended)
4. [Complete Debugging Workflows](#complete-debugging-workflows)
5. [MLX Lazy Evaluation](#mlx-lazy-evaluation-essential)
6. [Model Porting Guide](#model-porting-guide)
7. [Live Debugging Lessons Learned](#live-debugging-lessons-learned)

---

## Quick Start

### Start the REST API Server

```bash
cd /path/to/mflux
uvicorn mflux_debugger.fastapi_server:app --host 127.0.0.1 --port 8000
```

### Basic Workflow

```bash
# 1. Start debugging session
curl -X POST http://localhost:8000/debug/start \
  -H "Content-Type: application/json" \
  -d '{"script_path": "src/mflux_debugger/examples/debug_mflux.py"}'

# 2. Set breakpoint
curl -X POST http://localhost:8000/debug/breakpoint \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/file.py", "line": 100}'

# 3. Start execution (non-blocking for ML workloads!)
curl -X POST http://localhost:8000/debug/continue_async

# 4. Poll for breakpoint
curl http://localhost:8000/debug/status

# 5. When paused, inspect variables
curl http://localhost:8000/debug/location
curl -X POST http://localhost:8000/debug/evaluate \
  -H "Content-Type: application/json" \
  -d '{"expression": "latents.shape"}'

# 6. Terminate (auto-saves trace)
curl -X POST http://localhost:8000/debug/terminate
```

**Interactive API docs:** http://localhost:8000/docs

---

## Core Philosophy

### âš ï¸ Interactive Debugging Over Scripts

**CRITICAL:** When investigating models or comparing implementations, **USE THE DEBUGGER**, not temporary scripts!

**Why:**
- âœ… Inspects actual production code
- âœ… Can't check the wrong thing
- âœ… No maintenance burden
- âœ… Git handles versioning
- âœ… Trustworthy results

**Problems with scripts:**
- âŒ Can compare different computation points
- âŒ Hard to maintain, accumulate over time
- âŒ Can be written incorrectly
- âŒ Create dual implementations that drift

**Only write scripts if explicitly requested.**

### Example: Comparing Implementations (Good Approach)

âŒ **DON'T:**
```python
# Comparison script - might compare wrong things
def compare_encoders():
    mlx_enc = load_mlx()
    torch_enc = load_torch()
    # Easy to load wrong weights or compare wrong points
```

âœ… **DO:**
```
# Debug Session 1 (PyTorch reference)
Set breakpoint at pipeline_flux.py line 828
Inspect prompt_embeds.shape  # [1, 512, 4096]

# Debug Session 2 (MLX implementation)  
Set breakpoint at flux.py line 72
Inspect prompt_embeds.shape  # Verify matches reference
```

Traces are automatically saved for offline comparison!

---

## REST API Usage (Recommended)

### Why REST API over MCP?

âœ… **More stable** - HTTP is battle-tested  
âœ… **Language agnostic** - Any tool can use it  
âœ… **Better for automation** - Easy to script  
âœ… **Interactive docs** - Swagger UI built-in  

### Key Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/debug/start` | POST | Start session |
| `/debug/breakpoint` | POST | Set breakpoint |
| `/debug/breakpoint/remove` | POST | Remove breakpoint |
| `/debug/breakpoints` | GET | List all breakpoints |
| `/debug/breakpoints/clear` | POST | Clear all breakpoints |
| `/debug/continue_async` | POST | **Non-blocking continue** |
| `/debug/step/over` | POST | Step over line |
| `/debug/step/into` | POST | Step into function |
| `/debug/step/out` | POST | Step out of function |
| `/debug/evaluate` | POST | Evaluate expression |
| `/debug/location` | GET | Current location |
| `/debug/status` | GET | Check if paused |
| `/debug/variables` | GET | List variables |
| `/debug/terminate` | POST | End session + save trace |

### Async Execution (Essential for ML!)

**Problem:** Model loading takes 30-60 seconds â†’ HTTP timeout!

**Solution:** Async execution with polling

```bash
# Start execution (returns immediately)
curl -X POST http://localhost:8000/debug/continue_async

# Poll for status (every 2-5 seconds)
while true; do
  STATUS=$(curl -s http://localhost:8000/debug/status | jq -r '.data.state')
  if [ "$STATUS" = "paused" ]; then
    echo "Hit breakpoint!"
    break
  fi
  sleep 3
done

# Now inspect
curl http://localhost:8000/debug/location
```

---

## Complete Debugging Workflows

### Workflow 1: Debugging Diffusers (PyTorch)

```bash
# Start server
uvicorn mflux_debugger.fastapi_server:app --host 127.0.0.1 --port 8000 &

# Start session
curl -X POST http://localhost:8000/debug/start \
  -H "Content-Type: application/json" \
  -d '{"script_path": "src/mflux_debugger/examples/debug_diffusers.py"}'

# Set strategic breakpoints
curl -X POST http://localhost:8000/debug/breakpoint \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/diffusers/src/diffusers/pipelines/flux/pipeline_flux.py", "line": 828}'  # Prompt encoding

curl -X POST http://localhost:8000/debug/breakpoint \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/diffusers/src/diffusers/pipelines/flux/pipeline_flux.py", "line": 856}'  # Latent creation

# Start async execution
curl -X POST http://localhost:8000/debug/continue_async

# Wait for model loading (~45 seconds)
sleep 50

# Check if paused
curl http://localhost:8000/debug/location

# Inspect text embeddings
curl -X POST http://localhost:8000/debug/evaluate \
  -H "Content-Type: application/json" \
  -d '{"expression": "prompt_embeds.shape"}'
# Returns: [1, 512, 4096]

# Continue to next breakpoint
curl -X POST http://localhost:8000/debug/continue_async
sleep 2

# Inspect latents
curl -X POST http://localhost:8000/debug/evaluate \
  -H "Content-Type: application/json" \
  -d '{"expression": "latents.shape"}'
# Returns: [1, 256, 64]

# Terminate (saves trace automatically)
curl -X POST http://localhost:8000/debug/terminate
# Check: mflux_debugger/traces/debug_diffusers_YYYYMMDD_HHMMSS.json
```

### Workflow 2: Debugging MLX Implementation

```bash
# Start session
curl -X POST http://localhost:8000/debug/start \
  -H "Content-Type: application/json" \
  -d '{"script_path": "src/mflux_debugger/examples/debug_mflux.py"}'

# Set breakpoint (after model loads)
curl -X POST http://localhost:8000/debug/breakpoint \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/mflux/src/mflux/models/flux/variants/txt2img/flux.py", "line": 144}'  # VAE decode

# Start async execution
curl -X POST http://localhost:8000/debug/continue_async

# Poll for breakpoint (model loads for ~60 seconds)
for i in {1..30}; do
  STATUS=$(curl -s http://localhost:8000/debug/status | jq -r '.data.state')
  if [ "$STATUS" = "paused" ]; then
    echo "âœ… Paused at VAE decode!"
    break
  fi
  echo "[$i/30] Still loading..."
  sleep 3
done

# Inspect latents before VAE
curl -X POST http://localhost:8000/debug/evaluate \
  -H "Content-Type: application/json" \
  -d '{"expression": "latents.shape"}'
# Returns: [1, 16, 32, 32]

# Compare with PyTorch trace to verify match!

# Terminate
curl -X POST http://localhost:8000/debug/terminate
```

### Workflow 3: Managing Breakpoints in Loops

**Problem:** Breakpoint inside loop hits 4+ times (annoying!)

**Solution:** Remove breakpoint after first hit

```bash
# Set breakpoint in loop
curl -X POST http://localhost:8000/debug/breakpoint \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/file.py", "line": 944}'

# Continue until hit
curl -X POST http://localhost:8000/debug/continue_async

# ... wait for pause ...

# Inspect once
curl -X POST http://localhost:8000/debug/evaluate \
  -H "Content-Type: application/json" \
  -d '{"expression": "i"}'

# Remove breakpoint so we don't hit it again
curl -X POST http://localhost:8000/debug/breakpoint/remove \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/path/to/file.py", "line": 944}'

# Continue to next breakpoint (skips remaining loop iterations)
curl -X POST http://localhost:8000/debug/continue_async
```

---

## MLX Lazy Evaluation (Essential!)

### The Challenge

**MLX uses lazy evaluation** - computations are deferred until `mx.eval()`:

```python
# This doesn't compute anything!
latents = mx.random.normal(shape=[1, 256, 64])

# Computation happens here:
mx.eval(latents)  # NOW it executes
```

**Impact on debugging:**
- âŒ Breakpoint BEFORE `mx.eval()` â†’ tensor has no values
- âŒ `.mean()`, `.shape` â†’ also lazy
- âŒ Debugger shows nothing or hangs

### Solution: Debugger Auto-Handles This!

**Good news:** The lightweight debugger **automatically handles MLX lazy evaluation**!

- âœ… Selectively traces only user code
- âœ… Skips ML library internals
- âœ… Evaluates tensors automatically when inspecting
- âœ… No manual `mx.eval()` needed for debugging

### Best Breakpoint Locations for MLX

âœ… **GOOD** - After natural evaluation points:
```python
for t in time_steps:
    latents = self.transformer(...)
    latents = self.scheduler.step(...)
    mx.eval(latents)  # Already here for progress!
    # âœ… Set breakpoint here - tensor is evaluated
```

âŒ **BAD** - Before evaluation:
```python
latents = mx.random.normal(...)
# âŒ Breakpoint here - lazy, not computed yet
mx.eval(latents)
# âœ… Breakpoint here - now computed
```

---

## Model Porting Guide

### Working Backwards Strategy (Proven Successful)

**Problem:** Entire pipeline is hard to debug at once.

**Solution:** Implement and verify components **backwards**, from output to input.

```
Traditional (hard):
Text â†’ Latent â†’ Transformer â†’ VAE â†’ Image
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ implement everything â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                   then debug

Backwards (easy):
Text â† Latent â† Transformer â† VAE â† Image
                              â†‘
                         Start here!
```

**Implementation order:**

1. **VAE Decoder** (outputs images - easy to verify visually)
2. **Transformer last block** (verify numerically)
3. **Work backwards through transformer blocks**
4. **Text encoder**
5. **Latent creation**
6. **End-to-end check**

**Why this works:**
- Isolates each component
- Visual verification early (VAE produces images)
- Removes non-determinism by using saved inputs
- Builds confidence incrementally

### Verification Framework

**Tolerance levels:**
- `error < 1e-5` â†’ âœ… Clearly correct
- `1e-5 < error < 0.1` â†’ âš ï¸ Check visual output
- `error > 0.1` â†’ âŒ Clearly broken

**Don't reject implementations in the gray area without visual inspection!**

### Small Verified Chunks

âœ… **Verify each component thoroughly before moving on**  
âœ… **Don't accumulate uncertainty**  
âœ… **Use Git to revert if needed**  
âŒ **Don't doubt previously verified components**

---

## Live Debugging Lessons Learned

### Session 1: Path Normalization Bug (FIXED!)

**Problem:** Breakpoints not hitting in Diffusers pipeline

**Root cause:** 
- Breakpoints stored with absolute paths: `/Users/.../diffusers/file.py`
- Frame paths were relative: `../diffusers/file.py`  
- Mismatch â†’ breakpoint never matched!

**Fix:** Normalize all paths to absolute before comparison

```python
# In check_breakpoint():
filename = str(Path(frame.f_code.co_filename).resolve())
```

**Result:** âœ… Breakpoints hit reliably!

### Session 2: Crash Resilience (IMPLEMENTED!)

**Problem:** Server crashed during debugging â†’ lost all trace data

**Root cause:** Traces only saved on explicit `terminate()` call

**Fix:** Incremental auto-save after every step

**Features added:**
- âœ… Auto-save after each breakpoint/evaluation
- âœ… Emergency save on unexpected exit (`atexit` handler)
- âœ… Timestamped trace files (never overwrite)
- âœ… Automatic session cleanup on restart

**Result:** Never lose debugging progress, even on crash!

### Session 3: Loop Breakpoints (NOW MANAGEABLE!)

**Problem:** Breakpoint in denoising loop hit 4x (once per step)

**Solution:** Added breakpoint management API

**New endpoints:**
- `POST /debug/breakpoint/remove` - Remove specific breakpoint
- `GET /debug/breakpoints` - List all breakpoints
- `POST /debug/breakpoints/clear` - Clear all at once

**Workflow:**
```bash
# Hit breakpoint once
curl -X POST http://localhost:8000/debug/continue_async

# Inspect
curl -X POST http://localhost:8000/debug/evaluate -d '{"expression": "i"}'

# Remove so we don't hit it again
curl -X POST http://localhost:8000/debug/breakpoint/remove \
  -d '{"file_path": "...", "line": 944}'

# Continue to next breakpoint (skips remaining iterations)
curl -X POST http://localhost:8000/debug/continue_async
```

### Session 4: Tensor Shape Verification

**Finding:** PyTorch and MLX implementations have **identical tensor shapes**!

| Stage | Tensor | Shape | Match |
|-------|--------|-------|-------|
| Text encoding (T5) | prompt_embeds | `[1, 512, 4096]` | âœ… |
| Text encoding (CLIP) | pooled_embeds | `[1, 768]` | âœ… |
| Initial latents | latents | `[1, 256, 64]` | âœ… |
| Unpacked latents | latents | `[1, 16, 32, 32]` | âœ… |

**This proves the architecture is correctly ported!**

### Session 5: Trace Recording

**Feature:** Automatic trace recording for offline analysis

**What's captured:**
- All breakpoints set
- All execution steps with location, code context, call stack
- All evaluations with results
- Timestamps for performance analysis

**Trace format:**
```json
{
  "session_id": "20251024_133020",
  "script_path": "/path/to/debug_mflux.py",
  "start_time": "2025-10-24T13:30:20...",
  "end_time": "2025-10-24T13:32:02...",
  "breakpoints": [
    {"file": "...", "line": 144, "condition": null}
  ],
  "steps": [
    {
      "step": 1,
      "step_type": "continue_async",
      "location": {"file": "...", "line": 144, "function": "generate_image"},
      "code_context": {"before": [...], "current": [...], "after": [...]},
      "call_stack": [...],
      "variables": {...}
    },
    {
      "step": 2,
      "step_type": "evaluation",
      "expression": "latents.shape",
      "result": "[1, 16, 32, 32]"
    }
  ],
  "total_steps": 6
}
```

**Location:** `mflux_debugger/traces/debug_NAME_TIMESTAMP.json`

---

## Tips for AI Agents

### Do's âœ…

- Use async execution (`/debug/continue_async`) for ML workloads
- Poll status every 2-5 seconds during long operations
- Remove breakpoints after first hit in loops
- Start with PyTorch (reference) before debugging MLX
- Check trace files for offline analysis
- Use `/debug/evaluate` for quick inspections

### Don'ts âŒ

- Don't use blocking `/debug/continue` for model loading (timeout!)
- Don't set breakpoints in ML library code (slow!)
- Don't leave breakpoints in tight loops
- Don't compare different computation points across implementations
- Don't write temporary scripts unless explicitly requested

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transport Layer (choose one)       â”‚
â”‚  - FastAPI REST API (recommended)   â”‚
â”‚  - MCP Server (Cursor integration)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DebuggerService (business logic)   â”‚
â”‚  - Session management                â”‚
â”‚  - Breakpoint management             â”‚
â”‚  - Variable formatting               â”‚
â”‚  - Rich context capture              â”‚
â”‚  - Async execution                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LightweightDebugger (core)         â”‚
â”‚  - Selective tracing (user code)    â”‚
â”‚  - Library skipping (torch, mlx)    â”‚
â”‚  - Thread-safe state caching        â”‚
â”‚  - MLX auto-evaluation               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TraceRecorder (persistence)        â”‚
â”‚  - Incremental auto-save             â”‚
â”‚  - Emergency save on crash           â”‚
â”‚  - Rich context capture              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Performance

**Overhead comparison:**

| Operation | Debugpy | Lightweight | Speedup |
|-----------|---------|-------------|---------|
| Startup | 2-3s | ~0.1s | 20-30x |
| Model loading | 60s+ (often hangs) | Normal speed | âˆ |
| Step over | ~0.5s | ~0.01s | 50x |
| Variable inspection | ~0.2s | ~0.001s | 200x |

**Key advantage:** Doesn't slow down model loading because it skips ML library internals!

---

## Requirements

```bash
# Core
uv pip install fastapi uvicorn

# For PyTorch debugging (reference)
uv pip install torch diffusers

# For MLX debugging
# (MLX is pre-installed on mflux project)
```

---

## Troubleshooting

### Breakpoint not hitting?

**Check:**
1. Is the path absolute? Use absolute paths for `file_path`
2. Is the script running? Check `/debug/status`
3. Is the line executable? Set breakpoint on actual code, not comments/blank lines

### "Still running" after 60 seconds?

**This is normal for model loading!** Use async execution:
```bash
curl -X POST http://localhost:8000/debug/continue_async
# Then poll every few seconds
curl http://localhost:8000/debug/status
```

### Variables show `null`?

**For MLX:** Breakpoint might be before evaluation. Move to after `mx.eval()` call.

### Lost trace after crash?

**Good news:** Traces are auto-saved after every step! Check:
```bash
ls -lt mflux_debugger/traces/*.json | head -1
```

---

## Credits

Developed through live debugging sessions comparing FLUX.1 PyTorch (Diffusers) and MLX implementations.

**Version 0.2.0** includes lessons learned from:
- Path normalization debugging
- Crash resilience testing  
- Loop breakpoint management
- Async execution for ML workloads
- Tensor shape verification across frameworks

---

**Happy Debugging! ğŸ› â†’ âœ…**
